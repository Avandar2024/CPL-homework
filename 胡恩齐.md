<!doctype html><div class="lake-content" typography="classic"><h3 id="UiD4p"><span class="ne-text">必做任务1</span></h3><p id="uc3a9ff97" class="ne-p"><span class="ne-text">我的思路是：</span></p><ul class="ne-ul"><li id="u68dd3ae3" data-lake-index-type="0"><span class="ne-text">回忆我会用到的一些信息检索方法</span></li></ul><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="uf04b2bbf" data-lake-index-type="0"><span class="ne-text">根据自己之前使用的经历，比较每种方法的优劣</span></li></ul></ul><ul class="ne-ul"><li id="u3c9dab40" data-lake-index-type="0"><span class="ne-text">问问别人，你们平时都如何经行信息检索</span></li></ul><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="u5621f6a8" data-lake-index-type="0"><span class="ne-text">别人有时候会提出一些新的信息检索办法。比如我之前看不懂别人python代码里面的语句到底是什么意思，这在搜索引擎中进行搜索又是一件很麻烦的事情，但是上次活动和蔡子奇交流了之后，他告诉我可以直接把这句话丢给大模型，这是打开了我搜索信息的新思路。</span></li></ul></ul><ul class="ne-ul"><li id="u4b91159b" data-lake-index-type="0"><span class="ne-text">对于每一类搜索办法，在大脑里留个印象，什么办法适合于搜索什么信息。</span></li></ul><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="u37278215" data-lake-index-type="0"><span class="ne-text">比如要去搜往年卷，那肯定是qq群里面更多</span></li><li id="uaf17b27f" data-lake-index-type="0"><span class="ne-text">比如要去搜一个小技巧（如，如何配置vscode环境）比较适合找个视频网站一步一步跟着做</span></li><li id="u678008f7" data-lake-index-type="0"><span class="ne-text">比如有一些问题，但是又不太好泛化描述，可以试试LLM</span></li></ul></ul><ul class="ne-ul"><li id="u2220d436" data-lake-index-type="0"><span class="ne-text">多交交朋友，有时候可以站在巨人的肩膀上，别人搜到的信息直接拿来用（doge）</span></li></ul><p id="u85b4348b" class="ne-p"><span class="ne-text">任务耗时：10分钟</span></p><h3 id="EW4OH"><span class="ne-text">必做任务2</span></h3><p id="u0522d62b" class="ne-p"><span class="ne-text">我的感受是：</span></p><p id="u7d37799f" class="ne-p"><span class="ne-text" style="color: rgb(51, 51, 51); font-size: 16px">还是能学到一些东西的，看了一半之后我已经发现自己的盒人能力大大增强。</span></p><p id="u189faca7" class="ne-p"><span class="ne-text" style="color: rgb(51, 51, 51); font-size: 16px">感觉自己之前的一些信息检索策略比较古老，以及一些新出现的工具没有很快的适应使用。</span></p><p id="u4c8bcafe" class="ne-p"><span class="ne-text">任务耗时：50分钟</span></p><h3 id="UDYiT"><span class="ne-text">必做任务3</span></h3><p id="u29172084" class="ne-p"><span class="ne-text">我的感受是：挺好的</span></p><p id="u6836235d" class="ne-p"><span class="ne-text"></span></p><p id="u8a9ee826" class="ne-p"><span class="ne-text"></span></p><p id="uf3a67847" class="ne-p"><span class="ne-text"></span></p><p id="ue68920e8" class="ne-p"><span class="ne-text">任务耗时：0.1分钟</span></p><h3 id="boDqP"><span class="ne-text">必做任务4</span></h3><p id="u2be66e5f" class="ne-p"><span class="ne-text">我学习了：</span></p><p id="u4d2adcf0" class="ne-p"><span class="ne-text"></span></p><p id="uaf440b5e" class="ne-p"><span class="ne-text"></span></p><p id="u9744044b" class="ne-p"><span class="ne-text"></span></p><p id="u829218a1" class="ne-p"><span class="ne-text">任务耗时：XXX分钟</span></p><h3 id="eTKut"><span class="ne-text">分支任务：</span></h3><h4 id="zivjw"><span class="ne-text">我选择完成：</span></h4><article class="lake-columns" style="display: flex"><article class="lake-column-item" style="flex: 0.33333333000000004"><ul class="ne-tl"><li id="uc4310cb5" data-lake-index-type="0"><input type="checkbox"><span class="ne-text">分支任务1</span></li><li id="u3a738b00" data-lake-index-type="0"><input type="checkbox"><span class="ne-text">分支任务2</span></li></ul></article><article class="lake-column-item" style="flex: 0.33333333000000004"><ul class="ne-tl"><li checked="true" id="u4dfcf3ee" data-lake-index-type="0"><input type="checkbox" checked="true"><span class="ne-text">分支任务3</span></li><li id="ub67fd927" data-lake-index-type="0"><input type="checkbox"><span class="ne-text">分支任务4</span></li></ul></article><article class="lake-column-item" style="flex: 0.33333333000000004"><ul class="ne-tl"><li id="u566fa63f" data-lake-index-type="0"><input type="checkbox"><span class="ne-text">分支任务5</span></li><li checked="true" id="ud8d6b74f" data-lake-index-type="0"><input type="checkbox" checked="true"><span class="ne-text">分支任务6</span></li></ul></article></article><h4 id="nr7EL"><span class="ne-text">任务文档：</span></h4><p id="uab8b315c" class="ne-p"><span class="ne-text">用PBL的思路来思考这个问题：</span></p><ul class="ne-ul"><li id="u3c8d6934" data-lake-index-type="0"><span class="ne-text">什么是爬虫，爬虫在完成一件什么事-&gt;需要了解一些基本的名词和操作</span></li></ul><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="u22dae440" data-lake-index-type="0"><span class="ne-text">属于要简单了解的，毕竟我觉得像http这种东西搞的通通透透也没必要，但是还是有必要了解一下的，比较适合看看科普视频。</span></li><li id="uffcc6cd9" data-lake-index-type="0"><span class="ne-text">不学各种反爬措施，找个简单的视频跟着做做，基本的操作就知道了，对于一些函数和语句，用LLM来信息检索再合适不过了。</span></li></ul></ul><ul class="ne-ul"><li id="ua0af7fcb" data-lake-index-type="0"><span class="ne-text">爬虫的三个步骤（不就是分支345吗。。）</span></li></ul><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="u41d81d44" data-lake-index-type="0"><span class="ne-text">还是从实践中学比较合适，所以我找了一个爬豆瓣250的代码，并用LLM辅助，搞懂了每一句话在干一件什么事</span></li></ul></ul><ul class="ne-ul"><li id="u26a25ce9" data-lake-index-type="0"><span class="ne-text">将学习到的爬虫知识运用到实践中（获取一些数据）</span></li></ul><p id="u265d2e94" class="ne-p"><span class="ne-text"></span></p><p id="u19f81207" class="ne-p"><span class="ne-text">我先学习了一些爬虫的相关知识</span></p><ul class="ne-ul"><li id="ud8b6450c" data-lake-index-type="0"><span class="ne-text">url：访问网页的唯一链接</span></li><li id="u036310e3" data-lake-index-type="0"><span class="ne-text">http协议：类似于快递公司，规定诗句的传输方式</span></li><li id="uf3444a9c" data-lake-index-type="0"><span class="ne-text">浏览器发送请求给服务器，web服务器根据请求返回数据</span></li><li id="u2864ce19" data-lake-index-type="0"><span class="ne-text">requests.get()：模拟浏览器请求网页</span></li><li id="u4fd4e525" data-lake-index-type="0"><span class="ne-text">beautifulsoup是一个html或者xml的解析模块</span></li><li id="u2688f7a8" data-lake-index-type="0"><span class="ne-text">find_all()：根据标签名，获取BeautifulSoup对象中的节点</span></li></ul><p id="uf747a9eb" class="ne-p"><span class="ne-text">requests.get()的418错误：</span><a href="https://zhuanlan.zhihu.com/p/113331068" data-href="https://zhuanlan.zhihu.com/p/113331068" target="_blank" class="ne-link"><span class="ne-text">解决requests.get()状态码418问题 - 知乎</span></a></p><p id="u868cd24f" class="ne-p"><span class="ne-text">由于之前没有爬虫的基础，我觉得这是一个非常好的新手入门视频：</span></p><div id="reMZb" class="ne-thirdparty"><a href="https://player.bilibili.com/player.html?bvid=BV1WH4y177h6&amp;autoplay=0">https://player.bilibili.com/player.html?bvid=BV1WH4y177h6&amp;autoplay=0</a></div><p id="u7c39e5a1" class="ne-p"><br></p><p id="u1741c20f" class="ne-p"><span class="ne-text">这个视频解释了一些基本的爬虫操作，然后我试着弄懂了一份爬豆瓣250的代码。下面是我的一些尝试，以及爬豆瓣的代码：</span></p><pre data-language="python" id="ExLGS" class="ne-codeblock language-python"><code>import requests
from bs4 import BeautifulSoup
import re
import xlwt

baseurl = &quot;https://movie.douban.com/top250?start=&quot;

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'
}

#response = requests.get(url,headers = headers)

'''
with open('out.html', 'w', encoding='utf-8') as f:
    f.write(response.text)
'''
#encoding=‘utf-8'很重要，因为不是用gbk格式编写的

#html = response.text

#soup = BeautifulSoup(html,&quot;lxml&quot;)
#创建一个bs对象，解析html代码，&quot;lxml&quot;是解析器
#title_all = soup.find_all(name=&quot;span&quot;,class_=&quot;title&quot;)
'''
for title in title_all:
    titlestr = title.string
    print(titlestr)
'''

dataList = []
for i in range(0, 10):
    url = baseurl + str(i*25)
    response = requests.get(url,headers=headers)
    html = response.text

    soup = BeautifulSoup(html, &quot;html.parser&quot;)
    for item in soup.find_all(&quot;div&quot;, class_=&quot;item&quot;):
        data = []  # 存放一部电影的所有信息
        item = str(item)
        link = re.findall(r'&lt;a href=&quot;(.*)&quot;&gt;', item)[0]  # 链接
        #re.findall(pattern,string)返回的是一个列表
        data.append(link)
        image = re.findall(r'&lt;img.*src=&quot;(.*)&quot; .*/&gt;', item)[0]  # 图片
        data.append(image)
        titles = re.findall(r'&lt;span class=&quot;title&quot;&gt;(.*)&lt;/span&gt;', item)  # 片名
        data.append(titles[0])   # 添加中文名
        if len(titles) == 2:   # 添加外国名
            data.append(titles[1].replace(&quot;\\&quot;, &quot;&quot;))
        else:
            data.append(&quot; &quot;)
        rate = re.findall(r'&lt;span class=&quot;rating_num&quot;.*&gt;(.*)&lt;/span&gt;', item)[0]  # 评分
        data.append(rate)
        judge = re.findall(r'&lt;span&gt;(\d*)人评价&lt;/span&gt;', item)[0]  # 评级人数
        data.append(judge)
        inq = re.findall(r'&lt;span class=&quot;inq&quot;&gt;(.*)&lt;/span&gt;', item, re.S)  # 简述
        if len(inq) != 0:
            inq = inq[0].replace(&quot;。&quot;, &quot;&quot;)
            data.append(inq)
        else:
            data.append(&quot;&quot;)
        bd = re.findall(r'&lt;p class=&quot;&quot;&gt;(.*?)&lt;/p&gt;', item,  re.S)[0]    # 其他信息
        bd = re.sub('&lt;br/&gt;', &quot; &quot;, bd)
        bd = re.sub(&quot;/&quot;, &quot; &quot;, bd)
        bd = re.sub(&quot;\\n&quot;, &quot; &quot;, bd)
        bd = re.sub(r&quot;\xa0&quot;, &quot; &quot;, bd)
        data.append(bd.strip())
        dataList.append(data)

savePath = &quot;豆瓣250.xls&quot;
workbook = xlwt.Workbook(encoding=&quot;utf-8&quot;, style_compression=0)
worksheet = workbook.add_sheet(&quot;豆瓣top250&quot;, cell_overwrite_ok=True)
col = (&quot;电影详情链接&quot;, &quot;图片链接&quot;, &quot;影片中文名&quot;, &quot;影片英文名&quot;, &quot;评分&quot;, &quot;评价数&quot;, &quot;概况&quot;, &quot;相关信息&quot;)
for i in range(0, 8):
    worksheet.write(0, i, col[i])
for i in range(0, 250):
    data = dataList[i]
    for j in range(0, 8):
        worksheet.write(i+1, j, data[j])
workbook.save(savePath)

#print(title)</code></pre><p id="u187695bb" class="ne-p"><span class="ne-text">与之前不同的是，之前都是直接把别人的代码拿来当作一个工具用，这回我基本上是搞懂了每一句话的含义。同时也要感谢蔡子奇上周给我提供的信息检索方法：用LLM来询问不理解的语句，真的非常好用。</span></p><p id="u91c59cfe" class="ne-p"><span class="ne-text">对于一个之前没接触过爬虫的人来说，把这个代码搞懂还是有点成就感的。</span></p><p id="uf180a8e4" class="ne-p"><span class="ne-text"></span></p><h5 id="X1Krg"><span class="ne-text">爬教务网</span></h5><p id="ub4dfd9f6" class="ne-p"><span class="ne-text">服了，find_all()里面的class下划线是打在后面的，我打在前面一直没发现，debug了好久</span></p><p id="ueed3f021" class="ne-p"><span class="ne-text">学了一些正则表达式相关的操作，这篇还可以（</span><a href="https://zhuanlan.zhihu.com/p/139596371" data-href="https://zhuanlan.zhihu.com/p/139596371" target="_blank" class="ne-link"><span class="ne-text">https://zhuanlan.zhihu.com/p/139596371</span></a><span class="ne-text">）。没全学，够用了。</span></p><p id="u4a0cbda7" class="ne-p"><span class="ne-text">由于昨天的没有注意cac之前提到过的信息检索时候注意发表时间，我看的文章是用xlwt库，这玩意居然只能操作xls而不能操作xlsx，相当失败，今天又简答了解了一下openpyxl库</span></p><p id="u69c28b3b" class="ne-p"><a href="https://zhuanlan.zhihu.com/p/351998173" data-href="https://zhuanlan.zhihu.com/p/351998173" target="_blank" class="ne-link"><span class="ne-text">Python的Excel 神器 —— OpenPyXl - 知乎</span></a></p><p id="ub480e685" class="ne-p"><a href="https://zhuanlan.zhihu.com/p/259881853" data-href="https://zhuanlan.zhihu.com/p/259881853" target="_blank" class="ne-link"><span class="ne-text">爬虫——openpyxl模块 - 知乎</span></a></p><p id="ud0110174" class="ne-p"><span class="ne-text">openpyxl里面的append也太好用了吧！</span></p><pre data-language="python" id="z5qCP" class="ne-codeblock language-python"><code>from curl_cffi import requests
from bs4 import BeautifulSoup
import lxml
import re
from openpyxl import load_workbook
from openpyxl.workbook import Workbook

from test import savePath, workbook

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36 Edg/131.0.0.0'
}

url = &quot;https://jw.nju.edu.cn/ggtz/list.htm&quot;

response = requests.get(url, headers = headers)

print(response)

'''
with open('file/out.txt', 'w', encoding ='utf-8') as f:
    f.write(response.text)
'''

html = response.text
whole = BeautifulSoup(html, 'lxml')
seg = whole.find_all(&quot;ul&quot;,class_ = &quot;news_list list2&quot;)
seg = str(seg)
soup = BeautifulSoup(seg, 'lxml')

data_list = []
data_list.append(['类别','标题','日期','链接'])
for news in soup.find_all(&quot;li&quot;):
    '''
    item = str(item)
    with open('file/out.txt', 'w', encoding='utf-8') as f:
        f.write(item)
        f.write('\n\n\n')
    '''
    link_base = &quot;https://jw.nju.edu.cn/&quot;
    news = str(news)
    data = []
    kind = re.findall(r'&lt;div class=&quot;lj&quot;&gt;(.*)&lt;/div&gt;&lt;/span&gt;',news)[0]
    data.append(kind)
    title = re.findall(r'&lt;a href=&quot;.*&quot; target=&quot;_blank&quot; title=&quot;(.*)&quot;&gt;',news)[0]
    data.append(title)
    date = re.findall(r'&lt;span class=&quot;news_meta&quot;&gt;(.*)&lt;/span&gt;',news)[0]
    data.append(date)
    link = link_base + re.findall(r'&lt;a href=&quot;(.*)&quot; target=&quot;_blank&quot; title=&quot;.*&quot;&gt;',news)[0]
    data.append(link)
    data_list.append(data)

savePath = &quot;file/nju教务系统公告通知.xlsx&quot;
workbook  = Workbook()
sheet = workbook.active
for item in data_list:
    sheet.append(item)
workbook.save(savePath)</code></pre><p id="ubb720d91" class="ne-p"><span class="ne-text">效果还行</span></p><p id="u80b767d9" class="ne-p"><img src="https://cdn.nlark.com/yuque/0/2024/png/46961905/1732020847586-630cc5ac-6033-4b72-976d-e6d5a4bc3350.png" width="1226.6666666666667" id="uc38e73fa" class="ne-image"></p><h5 id="ezreN"><span class="ne-text">爬运动员网</span></h5><p id="u2c6b5597" class="ne-p"><span class="ne-text">最开始的网站应该是能爬的，稍微烦一点，学习了requests.post()的用法</span></p><pre data-language="python" id="xTXqK" class="ne-codeblock language-python"><code>import re
import requests
from bs4 import BeautifulStoneSoup
from openpyxl import load_workbook

def getimformation(name,certificate):
    url1 = &quot;https://ydydj.univsport.com/api/system/athlete/front-end-list&quot;
    data1 = {
        &quot;athleteRealName&quot;: name,
        &quot;certificateNo&quot;: certificate,
        &quot;pageNo&quot;:1
    }
    headers1 = {
        &quot;requestid&quot;:&quot;017939643192&quot;,
        &quot;user-agent&quot;:&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36 Edg/131.0.0.0&quot;
    }

    response1 = requests.post(url1,json = data1,headers = headers1)
    #print(response1.text)

    res1 = response1.text
    res1 = str(res1)
    tag = re.findall(r'&quot;total&quot;:(.*),&quot;list&quot;',res1)[0]
    if(tag == '0'):return tag
    athleteInfoId = re.findall(r'&quot;athleteInfoId&quot;:&quot;(.*)&quot;,&quot;certificateNo&quot;',res1)[0]
    #print(athleteInfoId)
    url2 = &quot;https://ydydj.univsport.com/api/system/athlete/front-end-detail&quot;
    data2 = {&quot;athleteInfoId&quot;: athleteInfoId}
    headers2 = {
        &quot;requestid&quot;:&quot;018040343253&quot;,
        &quot;user-agent&quot;:&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36 Edg/131.0.0.0&quot;
    }
    response2 = requests.post(url2,json = data2,headers = headers2)
    return response2

photo_url_base = &quot;https://ydydj.univsport.com/&quot;

workbook = load_workbook('file/athletes.xlsx')
sheet = workbook.active
#print(sheet[&quot;A1&quot;].value)
for i in range(2,272):
    name = sheet['A%s'%(i)].value
    certificate = sheet['B%s'%(i)].value
    res = getimformation(name,certificate)
    if(res == '0'):
        sheet['C%s' % (i)].value = &quot;信息有误&quot;
        continue
    text = res.text
    if(len(re.findall(r'&quot;oneInchPhoto&quot;:&quot;./(.*)&quot;,&quot;certificateNo&quot;',text))):
        photo_url = photo_url_base + re.findall(r'&quot;oneInchPhoto&quot;:&quot;./(.*)&quot;,&quot;certificateNo&quot;',text)[0]
    else:
        photo_url = photo_url_base + re.findall(r'&quot;oneInchPhoto&quot;:&quot;/(.*)&quot;,&quot;certificateNo&quot;', text)[0]
    ranktitle = re.findall(r'&quot;rankTitle&quot;:&quot;(.*)&quot;,&quot;subItemName&quot;',text)[0]
    da_xiang_mu = re.findall(r'&quot;subItemName&quot;:&quot;(.*)&quot;,&quot;smallItemName&quot;',text)[0]
    xiao_xiang_mu = re.findall(r'&quot;smallItemName&quot;:&quot;(.*)&quot;,&quot;grantUnitName&quot;',text)[0]
    org = re.findall(r'&quot;grantUnitName&quot;:&quot;(.*)&quot;,&quot;grantTime&quot;',text)[0]
    if(len(re.findall(r'&quot;eventGrade&quot;:&quot;\\t(.*)&quot;,&quot;eventName&quot;',text))):
        grade = re.findall(r'&quot;eventGrade&quot;:&quot;\\t(.*)&quot;,&quot;eventName&quot;',text)[0]
    else: grade = re.findall(r'&quot;eventGrade&quot;:&quot;(.*)&quot;,&quot;eventName&quot;',text)[0]
    time = re.findall(r'&quot;grantTime&quot;:&quot;(.*)&quot;,&quot;eventGrade&quot;',text)[0]
    sheet['C%s' % (i)].value = da_xiang_mu
    sheet['D%s' % (i)].value = xiao_xiang_mu
    sheet['E%s' % (i)].value = ranktitle
    sheet['F%s' % (i)].value = grade
    sheet['G%s' % (i)].value = org
    sheet['H%s' % (i)].value = time
    sheet['I%s' % (i)].value = photo_url
    print(name,&quot;获取成功！编号&quot;,i-1)
workbook.save('file/athletes.xlsx')</code></pre><p id="uf64fef80" class="ne-p"><img src="https://cdn.nlark.com/yuque/0/2024/gif/46961905/1732031090578-0ec3457d-fceb-4c94-a691-7002c067a39f.gif" width="1679.3333333333333" id="odOM7" class="ne-image"></p><p id="u0689d757" class="ne-p"><img src="https://cdn.nlark.com/yuque/0/2024/png/46961905/1732031248701-53032cb8-7305-4ae9-acde-7595fcff5c86.png" width="1706" id="u36510019" class="ne-image"></p><h5 id="kedOt"><span class="ne-text">爬竞赛获奖名单</span></h5><pre data-language="python" id="B6S0M" class="ne-codeblock language-python"><code>import requests
import re
import json
import openpyxl

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36 Edg/131.0.0.0'
}

url_base = &quot;https://api-portal.cyscc.org/portal/api/gs/findUser?MOLAWARE=1&amp;competitionId=94&amp;prizeId=6&amp;page=%s&amp;size=20&quot;

response = requests.get(url_base%4,headers = headers)
print(response.text)

data_list = []
#data_list.append([&quot;&quot;])
count = 1
while 1:
    url = url_base % count
    count += 1
    response = requests.get(url, headers = headers)
    content = response.text
    content = str(content)
    data = re.findall(r'&quot;data&quot;:{&quot;count&quot;:60,&quot;list&quot;:(.*)}}',content)[0]
    data = json.loads(data)
    if(not len(data)): break
    data_list.extend(data)

savePath = &quot;file/竞赛获奖名单.xlsx&quot;
workbook  = openpyxl.Workbook()
sheet = workbook.active
sheet.append(['姓名','学校名称','省份','竞赛名称','所获奖项'])
row = 1
for item in data_list:
    row += 1
    sheet[&quot;A%s&quot;%row] = item['name']
    sheet[&quot;B%s&quot;%row] = item['school']
    sheet[&quot;C%s&quot;%row] = item['province']
    sheet[&quot;D%s&quot;%row] = item['competitionName']
    sheet[&quot;E%s&quot;%row] = item['prizeName']
    print(item['name'],'加载成功！')
workbook.save(savePath)</code></pre><p id="u5cf412fc" class="ne-p"><img src="https://cdn.nlark.com/yuque/0/2024/png/46961905/1732106461980-e16d88f1-1ed7-4eb1-9a74-4577f94fca56.png" width="700.6666666666666" id="ubfa9737e" class="ne-image"></p><h5 id="UlGwF"><span class="ne-text">爬ppt模板下载链接</span></h5><p id="u5d6c38ec" class="ne-p"><span class="ne-text">相对实用一点，可以在程序中输入关键词，然后可以返回该关键词搜索结果的全部下载链接。</span></p><p id="u1db73bf9" class="ne-p"><span class="ne-text">由于本人是团支书，这周开团日活动要一个ppt模板，所以跟这个任务简单结合了一下。</span></p><pre data-language="python" id="rnwob" class="ne-codeblock language-python"><code>import requests
import re
from bs4 import BeautifulSoup
import openpyxl

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36 Edg/131.0.0.0'
}

url1 = &quot;https://www.ypppt.com/p/search.php?kwtype=1&amp;q=&quot;
print(&quot;搜索提示：请输入两三个字的核心关键词，能获得更准确的搜索结果。不要带有模板、PPT、素材、课件等词。&quot;)
key = input(&quot;请输入关键词：&quot;)

response = requests.get(url1 + key,headers = headers)
'''
with open('file/out.txt', 'w', encoding='utf-8') as f:
    f.write(response.text)
'''
content1 = response.text
content1 = str(content1)
soup1 = BeautifulSoup(content1, 'lxml')
content2 = soup1.find_all('ul',class_ = &quot;posts clear&quot;)
content2 = str(content2)
soup2 = BeautifulSoup(content2, 'lxml')

url_base = 'https://www.ypppt.com'
data_list = []
data_list.append(['标题','文件大小','下载量','下载链接'])
for item in soup2.find_all(&quot;li&quot;):
    data = []
    item = str(item)
    title = re.findall(r'target=&quot;_blank&quot;&gt;(.*)&lt;font color=&quot;red&quot;&gt;&lt;font color=&quot;red&quot;&gt;',item)[0]
    title += key
    title += re.findall(r'&lt;/font&gt;&lt;/font&gt;(.*)&lt;/a&gt;',item)[0]
    url2 = url_base + re.findall(r'&lt;a class=&quot;img_preview&quot; href=&quot;(.*)&quot; target=',item)[0]
    res2 = requests.get(url2,headers = headers)
    res2.encoding = 'utf-8'
    content3 = res2.text
    data.append(title)
    describe = re.findall(r'&lt;meta name=&quot;description&quot; content=&quot;(.*)&quot; /&gt;',content3)[0]
    size = re.findall(r'&lt;li class=&quot;sh&quot;&gt;&lt;i&gt;大小：&lt;/i&gt;(.*)&lt;/li&gt;',content3)[0]
    data.append(size)
    url3 = url_base + re.findall(r'&lt;a href=&quot;(.*)&quot; rel=&quot;nofollow&quot;',content3)[0]
    res3 = requests.get(url3,headers = headers)
    download = re.findall(r'&lt;li class=&quot;sh&quot;&gt;&lt;i&gt;下载：&lt;/i&gt;&lt;span id=&quot;countnum&quot;&gt;(.*)&lt;/span&gt;次&lt;/li&gt;',res3.text)[0]
    data.append(download)
    #data.append(describe)
    link = re.findall(r'&lt;li&gt;&lt;a href=&quot;(.*)&quot;&gt;下载地址',res3.text)[0]
    data.append(link)
    #print(data)
    data_list.append(data)
    print(title,&quot;获取成功！&quot;)

savePath = &quot;file/ppt模板.xlsx&quot;
workbook  = openpyxl.Workbook()
sheet = workbook.active
for item in data_list:
    sheet.append(item)
workbook.save(savePath)</code></pre><p id="u59a550df" class="ne-p"><img src="https://cdn.nlark.com/yuque/0/2024/gif/46961905/1732117087627-d8bc3891-938e-440e-bcb4-dc2a5b07dee4.gif" width="1680" id="u76f07663" class="ne-image"><img src="https://cdn.nlark.com/yuque/0/2024/png/46961905/1732117150044-ca8d62fb-59b1-4e96-abe2-ae898bf8bcd3.png" width="916.6666666666666" id="u42ab3a0b" class="ne-image"></p><p id="u771683ee" class="ne-p"><br></p><h4 id="leXWt"><span class="ne-text">任务耗时：</span></h4><p id="ue64f08cd" class="ne-p"><span class="ne-text">150+120+180+60+60分钟</span></p><h3 id="fHddQ"><span class="ne-text">本周活动情绪反馈</span></h3><p id="u6d431f79" class="ne-p"><span class="ne-text">文档写了一半才想起来还有模板这回事。。。。不过还好基本能对上</span></p><p id="uf134f4ff" class="ne-p"><span class="ne-text">这周的感受特别好！尤其是跟上次的任务比起来。上次那个语雀api的相关资料特别少，官方文档写的特别简(gou)约(shi)，导致任务很难推进，好几次红温了。但是这次的爬虫整体上经行还是比较顺利的（不过有可能是因为cac给我们找的网站都是他知道能爬的）学到了很多新东西。而且，自己写出来的程序确实有用，这是最有成就感的。</span></p><p id="u5a4d6b09" class="ne-p"><span class="ne-text">此外，特别鸣谢</span><span id="xdGdz" class="ne-mention"><a href="https://nova.yuque.com/zaccai">@蔡子奇</a></span><span class="ne-text">，代码上的问题问LLM是真有用，对于我这个python零基础的选手非常友好，在看代码写代码的同时也学到了很多python知识。</span></p><p id="u262b4051" class="ne-p"><span class="ne-text">自己能写python代码了，很开心qwq</span></p><p id="u5de8fbb0" class="ne-p"><span class="ne-text">花了好多时间，写的越开心越愿意写，求求数学和信物别挂啊QAQ</span></p><p id="u822f919e" class="ne-p"><span class="ne-text">（可阐述你的心得、体会、槽点、经验、感受...）</span></p><p id="uf856d2e3" class="ne-p"><br></p><p id="u1b2133cd" class="ne-p"><br></p></div>