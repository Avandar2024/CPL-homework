<!doctype html><div class="lake-content" typography="classic"><h3 id="UiD4p"><span class="ne-text">必做任务1</span></h3><p id="uc3a9ff97" class="ne-p"><span class="ne-text">我的思路是：</span></p><p id="ue60a1663" class="ne-p"><span class="ne-text">对于不同的搜索方式，有不同的注意点：</span></p><ul class="ne-ul"><li id="u04d02117" data-lake-index-type="0"><span class="ne-text">如果是询问他人，重要的是了解别人的专长和能力范围，不去问超过别人能里范围的问题。</span></li><li id="ua9a529b0" data-lake-index-type="0"><span class="ne-text">如果是使用搜索引擎，重要的是关键词。而关键词来源于对问题的拆解和对问题本质的把握。只有把握了问题的本质才能写出好的关键词。同时，搜索引擎的选用也很重要。</span></li><li id="u205b85b1" data-lake-index-type="0"><span class="ne-text">询问大模型的技巧是多次追问。要提出明确的指令，首先要明确自己要问什么。</span></li><li id="u141e7792" data-lake-index-type="0"><span class="ne-text">我很少在视频网站上检索信息。因为检索信息大多是需要即可使用的，而视频具有时长，一般做不到这一点。我更愿意在平时在刷这些视频网站，已及知乎等社交媒体时注意积累，万一有用再回头找。</span></li><li id="uc5fb0929" data-lake-index-type="0"><span class="ne-text">补充一点：注意信息渠道。对应的信息应该到对应的渠道去找。另外，我觉得搜索语言很重要。遇到疑难杂症我一般先翻译成英文再搜。</span></li></ul><p id="u85b4348b" class="ne-p"><span class="ne-text">任务耗时：10分钟（打字）</span></p><h3 id="EW4OH"><span class="ne-text">必做任务2</span></h3><p id="u0522d62b" class="ne-p"><span class="ne-text">我的感受是：好使。使用语法应该是谷歌浏览的特有功能（</span><span class="ne-text" style="text-decoration: line-through">反正百度没有）</span><span class="ne-text">世界第一的搜索引擎名不虚传。</span></p><p id="u7b8c8b18" class="ne-p"><span class="ne-text">嗯，其实我不太喜欢使用导航网站。良莠不齐，界面不清爽，有点烦人，不如直接上谷歌或先问LLM再搜谷歌。</span></p><p id="u4c8bcafe" class="ne-p"><span class="ne-text">任务耗时：10分钟</span></p><h3 id="UDYiT"><span class="ne-text">必做任务3</span></h3><p id="u29172084" class="ne-p"><span class="ne-text">我的感受是：</span></p><ul class="ne-ul"><li id="u384b27fc" data-lake-index-type="0"><span class="ne-text">不能一直在舒适区中生活，要挑战自己能够解决的问题，才能提升能力</span></li><li id="u65168267" data-lake-index-type="0"><span class="ne-text">代码能力是自己敲出来的，不是看书看出来或教出来的</span></li><li id="ueda3a41a" data-lake-index-type="0"><span class="ne-text">思维不能固化，不然容易走不出来</span></li><li id="u630938b0" data-lake-index-type="0"><span class="ne-text">写代码就是实现逻辑流的过程，所以debug最重要的是看程序有无正确执行编写者的意图</span></li></ul><p id="ue68920e8" class="ne-p"><span class="ne-text">任务耗时：10分钟</span></p><h3 id="boDqP"><span class="ne-text">必做任务4</span></h3><p id="u2be66e5f" class="ne-p"><span class="ne-text">我学习了：经过爬虫项目的洗礼，现在我对python不严格规定变量类型深恶痛绝。但也学习了有关迭代器等神奇的类型。</span></p><p id="u829218a1" class="ne-p"><span class="ne-text">任务耗时：10分钟</span></p><h3 id="eTKut"><span class="ne-text">分支任务：</span></h3><h4 id="zivjw"><span class="ne-text">我选择完成：</span></h4><article class="lake-columns" style="display: flex"><article class="lake-column-item" style="flex: 0.33333333000000004"><ul class="ne-tl"><li id="uc4310cb5" data-lake-index-type="0"><input type="checkbox"><span class="ne-text">分支任务1</span></li><li id="u3a738b00" data-lake-index-type="0"><input type="checkbox"><span class="ne-text">分支任务2</span></li></ul></article><article class="lake-column-item" style="flex: 0.33333333000000004"><ul class="ne-tl"><li checked="true" id="u4dfcf3ee" data-lake-index-type="0"><input type="checkbox" checked="true"><span class="ne-text">分支任务3</span></li><li id="ub67fd927" data-lake-index-type="0"><input type="checkbox"><span class="ne-text">分支任务4</span></li></ul></article><article class="lake-column-item" style="flex: 0.33333333000000004"><ul class="ne-tl"><li id="u566fa63f" data-lake-index-type="0"><input type="checkbox"><span class="ne-text">分支任务5</span></li><li checked="true" id="ud8d6b74f" data-lake-index-type="0"><input type="checkbox" checked="true"><span class="ne-text">分支任务6</span></li></ul></article></article><h4 id="nr7EL"><span class="ne-text">任务文档：</span></h4><h5 id="MzRde"><span class="ne-text">任务三的文档：</span></h5><p id="u2f11cb2f" class="ne-p"><span class="ne-text">检索和爬虫相关的东西的时候搜出来不少东西，分享下：</span></p><ul class="ne-ul"><li id="u0f69d707" data-lake-index-type="0"><span class="ne-text">爬虫其实有两种模式，一种直接发送请求，一种模拟人操纵浏览器。后者，其缺点是速度慢，内存占用高，优点是比较容易过反爬，容易过动态页面。selenium就是这种。前者实际应用中更常见，比如requests和scrapy，因为只进行数据交流，速度较快，但遇到动态页面需要对js code进行解码。</span></li><li id="u40e43869" data-lake-index-type="0"><span class="ne-text">而lxml和beatuifulsoup一样是解析器</span></li><li id="u927cc409" data-lake-index-type="0"><span class="ne-text">对应requests而言，只能发送请求，解析要靠别的库，比如bs。bs解析后可以用re正则表达式匹配。也可以直接对requests得到的html文件用xpath处理是比较主流的，其实xpath直接复制就好。</span><a href="https://www.w3schools.com/xml/xpath_intro.asp" data-href="https://www.w3schools.com/xml/xpath_intro.asp" target="_blank" class="ne-link"><span class="ne-text">xpath语法</span></a></li><li id="ub8b61c6e" data-lake-index-type="0"><span class="ne-text">安利一个可以集合了requests和selenium优点的库drissionpage。这个库有点是可以过滑动验证码之类反人类的操作。而且其玩法不只于爬虫。</span><a href="https://www.drissionpage.cn/" data-href="https://www.drissionpage.cn/" target="_blank" class="ne-link"><span class="ne-text">官方文档</span></a></li><li id="u57e0e109" data-lake-index-type="0"><span class="ne-text">贴一个关于反爬的原理。</span><a href="https://zhuanlan.zhihu.com/p/556854559" data-href="https://zhuanlan.zhihu.com/p/556854559" target="_blank" class="ne-link"><span class="ne-text">这些常见的反爬虫手段，你能攻破多少？</span></a></li><li id="ub74b9f57" data-lake-index-type="0"><span class="ne-text">requests库的第三方依赖，应该挺有用的。</span><a href="https://toolbelt.readthedocs.io/en/latest/dumputils.html" data-href="https://toolbelt.readthedocs.io/en/latest/dumputils.html" target="_blank" class="ne-link"><span class="ne-text">Utilities for Dumping Information About Responses</span></a></li></ul><h5 id="kuvIJ"><span class="ne-text">任务耗时：</span></h5><p id="ue64f08cd" class="ne-p"><span class="ne-text">不好说，我是边冲浪边看的。</span></p><h5 id="yRX1f"><span class="ne-text">任务六的文档：</span></h5><p id="uc358a811" class="ne-p"><span class="ne-text">没做完，插个眼先。</span></p><p id="u09a6e609" class="ne-p"><span class="ne-text">我注意到在浏览器的开发者工具中，是可以直接找到对应信息的url的。直接去请求这个url会得到json文件而不是html文件，所以实际上这么做是不需要bs等解析工具的。王浩宸同学就采用了这种思路。</span></p><p id="uf07b36ea" class="ne-p"><span class="ne-text">为了达到安利的目的，我都用drissionpage完成。</span></p><ul class="ne-ul"><li id="ubf84f48b" data-lake-index-type="0"><span class="ne-text">竞赛网站：（用到了drissionpage库的数据监听功能）</span></li></ul><p id="u92e7199a" class="ne-p"><span class="ne-text">鉴于我写文档的时候已经有两位同学做好了，而我也也复现成功了，下面给出基于浏览器模拟的思路：(</span><span class="ne-text" style="text-decoration: line-through">其实大部分是抄的</span><span class="ne-text">，不过做了优化)。</span></p><pre data-language="python" id="JIsv0" class="ne-codeblock language-python"><code>from DrissionPage import Chromium
import requests
import pandas as pd
import time
import random


url = 'https://www.cyscc.org/#/gs/winnerList?competitionId=96&amp;prizeId=2'
tab = Chromium().latest_tab
tab.listen.start('https://api-portal.cyscc.org/portal/api/gs/findUser?MOLAWARE')  # 开始监听，指定获取包含该文本的数据包
tab.get(url)  # 访问网址
tab('xpath:/html/body/div/div/div[2]/div/div[3]/div/div[2]/div[13]').click()

url_list = []
for i in range(6):
    random_time = random.random()
    time.sleep(random_time)# 随机等待时间,更像人类操作
    tab('tag:i@@class=el-icon el-icon-arrow-right').click(by_js=True)
    res = tab.listen.wait()
    url_list.append(str(res.url))
tab.close()
url_list.pop(0)

def into_excel(json_data,file_path):
    if not json_data:
        print('invalid input')
        return
    title_list = list(json_data[0].keys())
    rows = []
    for row in json_data:
        rows.append(list(row.values()))
    df = pd.DataFrame(rows, columns=title_list)
    df.to_excel(file_path, index=False)

headers = {
    &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36&quot;
}
all_data = []
for url in url_list:
    random_time = random.random()
    time.sleep(random_time)  # 随机等待时间,更像人类操作
    response = requests.get(url=url, headers=headers)
    data = response.json()
    data_dic = dict(data)
    data_list = data_dic['data']['list']
    key_to_remove = ['id','sex']
    cleaned_data = [{k:v for k,v in item.items() if k not in key_to_remove} for item in data_list]
    all_data.extend(cleaned_data)
into_excel(all_data,'jingsai.xlsx')</code></pre><ul class="ne-ul"><li id="u2b0650f3" data-lake-index-type="0"><span class="ne-text">运动员网站（用到了dp的模拟交互功能）</span></li></ul><pre data-language="python" id="yYBit" class="ne-codeblock language-python"><code>from openpyxl import load_workbook
from DrissionPage import Chromium

def getimformation(name,certificate):
    url = 'https://ydydj.univsport.com/level/'
    tab = Chromium().latest_tab
    tab.get(url)
    tab.ele('xpath://*[@id=&quot;app&quot;]/div/div[1]/div[2]/div[2]').input(certificate)#填证书号
    tab.ele('xpath://*[@id=&quot;app&quot;]/div/div[1]/div[3]/div[2]').input(name)#填姓名
    tab.ele('xpath://*[@id=&quot;app&quot;]/div/div[1]/div[6]').click()#点击查询
    tab.ele('xpath://*[@id=&quot;app&quot;]/div/div[2]/div[3]/div[1]/div[1]/div/table').click()#第二次点击
    tab.wait(0.1)#等待动态渲染完成
    content =dict()
    try:
        content['project'] = tab.ele('xpath://*[@id=&quot;app&quot;]/div/div[2]/div[3]/div[3]/div[4]/div[2]').text
        content['level'] = tab.ele('xpath://*[@id=&quot;app&quot;]/div/div[2]/div[3]/div[3]/div[3]/div[2]').text
        content['rank'] = tab.ele('xpath://*[@id=&quot;app&quot;]/div/div[2]/div[3]/div[3]/div[8]/div[2]').text
        content['awarded_by'] = tab.ele('xpath://*[@id=&quot;app&quot;]/div/div[2]/div[3]/div[3]/div[12]/div[2]').text
        content['date'] = tab.ele('xpath://*[@id=&quot;app&quot;]/div/div[2]/div[3]/div[3]/div[11]/div[2]').text
        content['image_url'] = tab.ele('xpath://*[@id=&quot;app&quot;]/div/div[2]/div[3]/div[3]/div[14]/img').link
    except:
        return 0
    return content
# print(getimformation('邓嘉禄','CFA2020030010014'))


workbook = load_workbook('demo.xlsx')
sheet = workbook.active
print(sheet[&quot;A1&quot;].value)
for i in range(2,272):
    name = sheet['A%s'%(i)].value
    certificate = sheet['B%s'%(i)].value
    res = getimformation(name,certificate)
    if(str(res) == '0'):
        sheet['C%s' % (i)].value = &quot;信息有误&quot;
        continue
    sheet['C%s' % (i)].value = res['project']
    sheet['D%s' % (i)].value = res['level']
    sheet['E%s' % (i)].value = res['rank']
    sheet['F%s' % (i)].value = res['awarded_by']
    sheet['G%s' % (i)].value = res['date']
    sheet['H%s' % (i)].value = res['image_url']
    print(name,&quot;获取成功！编号&quot;,i-1)
workbook.save('demo.xlsx')</code></pre><p id="ub69c5efb" class="ne-p"><span class="ne-text">贴个成果：</span></p><div id="HHEsU" class="ne-localdoc"><a href="https://nova.yuque.com/attachments/yuque/0/2024/xlsx/49698476/1732123035272-e1afb0b1-1074-41a6-ba73-60f73e910e78.xlsx">demo.xlsx</a></div><p id="uce7cec6a" class="ne-p"><span class="ne-text">自己试试，你就知道有多慢了。</span></p><ul class="ne-ul"><li id="u4a0a5041" data-lake-index-type="0"><span class="ne-text">PPT网站（其实dp也可以发送正常的request请求）</span></li></ul><pre data-language="python" id="EZKRC" class="ne-codeblock language-python"><code>from DrissionPage import SessionPage,SessionOptions
headers = {
    &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36&quot;  # 如果需要的话
}
if __name__ == '__main__':
    target = 'https://www.ypppt.com/moban/'
    page=SessionPage()
    page.get(url = target,headers=headers)
    lists = page.eles('@href:/article')
    for element in lists:
        url1 = element.link
        page.get(url = url1,headers=headers)
        link = page.ele('@class=down-button')
        url2 = link.link
        page.get(url = url2,headers=headers)
        dl_links = page.eles('tag:a@@text():下载地址')
        for dl_link in dl_links:
            with open(f&quot;./urls.txt&quot;,&quot;a&quot;) as file:
                file.write(dl_link.link+&quot;\n&quot;)</code></pre><p id="u60315601" class="ne-p"><br></p><div id="r1KIY" class="ne-localdoc"><a href="https://nova.yuque.com/attachments/yuque/0/2024/txt/49698476/1732186028260-d2785319-9e4a-43db-bcab-876e677d4e50.txt">urls.txt</a></div><h3 id="fHddQ"><span class="ne-text">本周活动情绪反馈</span></h3><p id="u822f919e" class="ne-p"><span class="ne-text">说一下我爬运动员网站的感受：</span></p><p id="ub9cfeb53" class="ne-p"><span class="ne-text" style="color: #DF2A3F; background-color: #FBDE28">我要问候给那个网站做反爬的人。</span></p><p id="uae9166b6" class="ne-p"><span class="ne-text">我用到反爬原理是模拟人操作浏览器。第一次操作得到了空白表格，我以为是页面元素定位错误，按f12查看了好久，觉得自己对页面结构的理解太肤浅了。于是丢给chatgpt，它分析的头头是道。我改改拿来，还是不行。此时已经过去了一个小时。一个偶然的机会，我又真人做了一次查询操作，忽然意识到：</span></p><p id="u26bba913" class="ne-p"><span class="ne-text">我的模拟程序少了一步点击。</span><span class="ne-text" style="color: #DF2A3F; background-color: #FBDE28">在点击搜索框后不会直接加载B区域的信息，需要再点击一下加载出的A。</span></p><p id="u1a3e5bf2" class="ne-p"><span class="ne-text">猫猫听完都死了。</span></p><p id="u4b14b5d3" class="ne-p"><img src="https://cdn.nlark.com/yuque/0/2024/png/49698476/1732116410413-dbb95368-ed5a-40af-b9f5-0e74ed66ecc8.png" width="1357" id="u067660c2" class="ne-image"></p><p id="u8112f097" class="ne-p"><br></p><p id="uf856d2e3" class="ne-p"><br></p><p id="u1b2133cd" class="ne-p"><br></p></div>