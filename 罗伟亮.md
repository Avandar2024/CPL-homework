<!doctype html><div class="lake-content" typography="classic"><h2 id="UNMlZ"><span class="ne-text">如何提升自己的信息检索能力，那么围绕这个问题，具体可以怎么做呢？</span></h2><h3 id="i1RAR"><span class="ne-text">任务一</span></h3><ul class="ne-ul"><li id="ub3794087" data-lake-index-type="0"><span class="ne-text">搜b站，youtube，知乎</span></li></ul><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="u13391144" data-lake-index-type="0"><span class="ne-text">在b站，youtube上寻找播放量较高但无标题党的视频，粉丝多的up</span></li><li id="ua33fb237" data-lake-index-type="0"><span class="ne-text">多看看评论区里网友对视频、文章的评价</span></li></ul></ul><p id="ub9636681" class="ne-p"><span class="ne-text">任务耗时：5min</span></p><h3 id="fVM5a"><span class="ne-text">任务二（</span><span class="ne-text" style="color: #ED740C">看完这个视频后的感受</span><span class="ne-text">）</span></h3><ul class="ne-ul"><li id="u492c087c" data-lake-index-type="0"><span class="ne-text">“</span><span class="ne-text" style="background-color: rgba(255, 255, 255, 0)">檀东东·Tango</span><span class="ne-text">”是我高三时在YouTube发现，但我一直觉得他恰饭推流得太明显了（包括这个视频</span><span class="ne-text">😂</span><span class="ne-text">），而且我之前觉得他的理论有些大而空，但大学后这些方法论会逐步派上用常。</span></li><li id="u15e8cee6" data-lake-index-type="0"><span class="ne-text">这个视频分为四部分：</span></li></ul><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="ud576ab22" data-lake-index-type="0"><strong><span class="ne-text">搜信息资源</span></strong><span class="ne-text">：他讲了很多Google语法，但我认为重点在于搜索的思维方式而非浅显的语法。</span><img src="https://cdn.nlark.com/yuque/0/2024/png/50039808/1731908953888-6f94227d-2018-49c5-870b-24df1f3ff0e1.png" width="444.7537841796875" id="u35eda7c7" class="ne-image"></li><li id="ubedbe864" data-lake-index-type="0"><strong><span class="ne-text">知识技能</span></strong><span class="ne-text">：视频告诉我们要先对知识技能分类，分别用不同的方式寻找；以及还有查找类似网站的</span><strong><span class="ne-text">similarsites</span></strong></li><li id="u20053c00" data-lake-index-type="0"><strong><span class="ne-text">素材文件</span></strong><span class="ne-text">：让我比较新奇的是那个在youtube.com前面加9x来下视频（没想到下视频这么方便</span><span class="ne-text">😂</span><span class="ne-text">）</span></li><li id="uc9b4e7eb" data-lake-index-type="0"><strong><span class="ne-text">工具软件</span></strong><span class="ne-text">：alternativeto.net寻找相似工具；善用搜索引擎寻找best software/extention</span></li></ul></ul><p id="u8466cc75" class="ne-p"><span class="ne-text">任务耗时1.5h（看视频加打字）</span></p><h3 id="Rhwy2"><span class="ne-text">任务三</span></h3><ul class="ne-ul"><li id="u40e96ffb" data-lake-index-type="0"><span class="ne-text">这两个视频阐述了提升编程能力的方法论。视频突出了“实践”的重要性。我们学习编程语言最终都是在计算机世界中创造事物。只有通过实践才能真正掌握编程知识与方法。</span></li></ul><h3 id="eTKut"><span class="ne-text">分支任务：</span></h3><h4 id="zivjw"><span class="ne-text">我选择完成：</span></h4><article class="lake-columns" style="display: flex"><article class="lake-column-item" style="flex: 0.33333333000000004"><ul class="ne-tl"><li id="uc4310cb5" data-lake-index-type="0"><input type="checkbox"><span class="ne-text">分支任务1</span></li><li id="u3a738b00" data-lake-index-type="0"><input type="checkbox"><span class="ne-text">分支任务2</span></li></ul></article><article class="lake-column-item" style="flex: 0.33333333000000004"><ul class="ne-tl"><li id="u4dfcf3ee" data-lake-index-type="0"><input type="checkbox"><span class="ne-text">分支任务3</span></li><li id="ub67fd927" data-lake-index-type="0"><input type="checkbox"><span class="ne-text">分支任务4</span></li></ul></article><article class="lake-column-item" style="flex: 0.33333333000000004"><ul class="ne-tl"><li id="u566fa63f" data-lake-index-type="0"><input type="checkbox"><span class="ne-text">分支任务5</span></li><li checked="true" id="ud8d6b74f" data-lake-index-type="0"><input type="checkbox" checked="true"><span class="ne-text">分支任务6</span></li></ul></article></article><p id="u402a8aa8" class="ne-p"><br></p><h3 id="px0Uj"><span class="ne-text">分支6</span></h3><h4 id="CUchA"><span class="ne-text">1</span></h4><p id="u6db5d203" class="ne-p"><span class="ne-text">我一开始傻了，由于公告通知这个板块是动态渲染，我死磕在 selenium 模块上模拟虚拟浏览器。结果突然想到可以直接点进“</span><strong><span class="ne-text">更多</span></strong><span class="ne-text">”按钮再爬</span><span class="ne-text">😂</span><span class="ne-text">。算了，就当学习edgedriver来绕开反爬虫。</span></p><pre data-language="python" id="QjMIG" class="ne-codeblock language-python"><code>from bs4 import BeautifulSoup
import requests
from curl_cffi import requests
import openpyxl


nju = &quot;https://jw.nju.edu.cn&quot;
r = requests.get(nju,impersonate=&quot;chrome101&quot;)
r.encoding = 'utf-8'
html = r.text
soup = BeautifulSoup(html, &quot;lxml&quot;)
more = soup.find(name = &quot;a&quot;,class_ = &quot;w41_more&quot;)
morelink = more.get(&quot;href&quot;)
morelink = nju + morelink
rm = requests.get(morelink,impersonate=&quot;chrome101&quot;)
rm.encoding = 'utf-8'
htmlm = rm.text
soupm = BeautifulSoup(htmlm, &quot;lxml&quot;)
workbook = openpyxl.Workbook()
sheet = workbook.active
sheet['A1'] = '标签'
sheet['B1'] = '发布日期'
sheet['C1'] = '公告标题'
index = 2 
ul = soupm.find(name = &quot;ul&quot;,class_ = &quot;news_list list2&quot;)
list = ul.find_all(name = &quot;li&quot;)
for i in list:
    sheet[f&quot;A{index}&quot;] = i.find(name = &quot;div&quot;,class_ = &quot;lj&quot;).text
    sheet[f&quot;B{index}&quot;] = i.find(name = &quot;span&quot;,class_ = &quot;news_meta&quot;).text
    sheet[f&quot;C{index}&quot;] = i.find(name = &quot;a&quot;).text
    index = index + 1

workbook.save('./1117-1123/information.xlsx')</code></pre><p id="uc7b07abd" class="ne-p"><span class="ne-text">成果：</span><img src="https://cdn.nlark.com/yuque/0/2024/png/50039808/1732014156678-5815dc9e-7257-4b11-be04-7297a2a13811.png" width="892" id="ua66b9027" class="ne-image"></p><h4 id="FBoJq"><span class="ne-text">2</span></h4><p id="u419faa73" class="ne-p"><span class="ne-text">终于在gpt的帮助下完工了</span><span class="ne-text">😂</span></p><p id="u6ff41cd6" class="ne-p"><span class="ne-text">重点就是找开发者模式中网络部分json文件，然后看请求载体和响应内容，根据python的报错来添加header元素（需要requestid）</span></p><pre data-language="python" id="KnAl4" class="ne-codeblock language-python"><code>import requests
import uuid
from openpyxl import load_workbook

# 定义目标URL
url1 = &quot;https://ydydj.univsport.com/api/system/athlete/front-end-list&quot;
url2 = &quot;https://ydydj.univsport.com/api/system/athlete/front-end-detail&quot;

# 加载 Excel 文件
wb2 = load_workbook('./1117-1123/demo.xlsx')
sheet = wb2.active

index = 2
while index &lt;= 271:
    # 获取单元格值
    name = sheet[f'A{index}'].value
    num = sheet[f'B{index}'].value

    # 检查空值
    if not name or not num:
        print(f&quot;Row {index} has missing data. Skipping...&quot;)
        index += 1
        continue

    # 生成 requestId
    request_id = str(uuid.uuid4())

    # 构造请求载荷和头
    payload1 = {&quot;pageNo&quot;: 1, &quot;certificateNo&quot;: num, &quot;athleteRealName&quot;: name, &quot;idCard&quot;: &quot;&quot;}
    headers = {
        &quot;Content-Type&quot;: &quot;application/json&quot;,
        &quot;user-agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36&quot;,
        &quot;requestId&quot;: request_id,
    }

    try:
        # 请求第一个接口
        response1 = requests.post(url1, json=payload1, headers=headers)
        response1.raise_for_status()
        data1 = response1.json()

        # 检查数据结构
        if not data1.get('data') or not data1['data'].get('list'):
            print(f&quot;No data for Row {index}. Skipping...&quot;)
            index += 1
            continue

        # 写入数据
        sheet[f'C{index}'] = data1['data']['list'][0]['item']
        sheet[f'D{index}'] = data1['data']['list'][0]['rankTitle']

        infold = data1['data']['list'][0]['athleteInfoId']
        payload2 = {&quot;athleteInfoId&quot;: infold}

        # 请求第二个接口
        response2 = requests.post(url2, json=payload2, headers=headers)
        response2.raise_for_status()
        data2 = response2.json()

        # 写入更多数据
        sheet[f'C{index}'] = sheet[f'C{index}'].value + &quot; &quot; + data2['data'][&quot;smallItemName&quot;]
        sheet[f'E{index}'] = data2['data'][&quot;eventGrade&quot;]
        sheet[f'F{index}'] = data2['data'][&quot;grantUnitName&quot;]
        sheet[f'G{index}'] = data2['data'][&quot;grantTime&quot;]
        sheet[f'H{index}'] = &quot;https://ydydj.univsport.com/&quot; + data2['data'][&quot;oneInchPhoto&quot;][2:]

    except Exception as e:
        print(f&quot;Error at Row {index}: {e}&quot;)

    # 增加索引并保存文件
    index += 1
    wb2.save('./1117-1123/demo.xlsx')
</code></pre><p id="u104896b7" class="ne-p"><span class="ne-text">成果：</span><img src="https://cdn.nlark.com/yuque/0/2024/png/50039808/1732080456763-d4a95501-f5de-4424-9545-942097dff810.png" width="1264.8485031250534" id="u3346b37d" class="ne-image"></p><p id="u8f1b7c29" class="ne-p"><span class="ne-text">有些是查不到的需要做异常处理</span></p><h4 id="ebCsi"><span class="ne-text">3</span></h4><ol class="ne-list-wrap"><ol ne-level="1" class="ne-ol"><li id="ud5713019" data-lake-index-type="0"><span class="ne-text"> 一开始用get网址的方式发现有部分网页元素不在网站源代码中，搜了gpt结果误打误撞发现了开发者模式中网络功能的json文档，于是直接按照之前语雀api的方式下载数据</span></li><li id="ube274175" data-lake-index-type="0"><span class="ne-text">api的url样例： <br /></span><a href=" https://api-portal.cyscc.org/portal/api/gs/findUser?MOLAWARE=1&amp;page=1&amp;size=20&amp;province=%E4%B8%8A%E6%B5%B7" data-href=" https://api-portal.cyscc.org/portal/api/gs/findUser?MOLAWARE=1&amp;page=1&amp;size=20&amp;province=%E4%B8%8A%E6%B5%B7" target="_blank" class="ne-link"><span class="ne-text" style="background-color: rgba(255, 255, 255, 0); font-size: 12px">https://api-portal.cyscc.org/portal/api/gs/findUser?MOLAWARE=1&amp;page=1&amp;size=20&amp;province=%E4%B8%8A%E6%B5%B7</span></a><span class="ne-text" style="background-color: rgba(255, 255, 255, 0); font-size: 12px">，其中page代表页数，province后的乱码是 字符串中的特殊字符和非 ASCII 字符转换为 </span><code class="ne-code"><span class="ne-text" style="background-color: rgba(255, 255, 255, 0); font-size: 12px">%</span></code><span class="ne-text" style="background-color: rgba(255, 255, 255, 0); font-size: 12px"> 加十六进制的形式  ，使用这个代码转</span></li></ol></ol><pre data-language="python" id="wLqk1" class="ne-codeblock language-python"><code>import urllib.parse

# 原始字符串
original_str = &quot;上海&quot;
# URL 编码
encoded_str = urllib.parse.quote(original_str)
print(encoded_str)  # 输出：%E4%B8%8A%E6%B5%B7
</code></pre><ol class="ne-list-wrap"><ol start="3" ne-level="1" class="ne-ol"><li id="u54523fa3" data-lake-index-type="0"><span class="ne-text" style="background-color: rgba(255, 255, 255, 0); font-size: 12px">感谢周家超同学的openpyxl</span><a href="https://nova.yuque.com/bm61ur/unzgre/ar7ic1p9yakl7go6#rqGCl" data-href="https://nova.yuque.com/bm61ur/unzgre/ar7ic1p9yakl7go6#rqGCl" target="_blank" class="ne-link"><span class="ne-text" style="background-color: rgba(255, 255, 255, 0); font-size: 12px">教程</span></a></li></ol></ol><span style="margin-left: 2em"><pre data-language="python" id="TJsWG" class="ne-codeblock language-python"><code>from bs4 import BeautifulSoup
import requests
import json
import urllib.parse #字符编码转换
import openpyxl

import os


headers = {
    &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36&quot;  # 如果需要的话
}
province = str(input(&quot;请输入你想获取的省份：&quot;))
# 原始字符串

# URL 编码
encoded_str = urllib.parse.quote(province)
page = 1
workbook = openpyxl.Workbook()
sheet = workbook.active
sheet['A1'] = '学生姓名'
sheet['B1'] = '毕业学校'
sheet['C1'] = '奖项名称'
index = 2 
while(page&lt;6):
    url = f&quot;https://api-portal.cyscc.org/portal/api/gs/findUser?MOLAWARE=1&amp;page={page}&amp;size=20&amp;province={encoded_str}&quot;
    response = requests.get(url, headers=headers)
    if response.status_code == 200:
        data = response.json()
        lists = data['data']['list']
        for stu in lists:
            sheet[f'A{index}'] = stu[&quot;name&quot;]
            sheet[f'B{index}'] = stu[&quot;school&quot;]
            sheet[f'C{index}'] = f'{stu[&quot;competitionName&quot;]} {stu['prizeName']}'
            index = index + 1
    else:
        break
    page = page + 1
workbook.save('./1117-1123/data.xlsx')</code></pre></span><p id="u3fa91c15" class="ne-p"><span class="ne-text" style="font-size: 12px">成果：</span><img src="https://cdn.nlark.com/yuque/0/2024/png/50039808/1732014203015-a2cc54d5-8f5e-42d3-801d-fcd7afb3e41f.png" width="776" id="uaa18a8bc" class="ne-image"></p><h4 id="xrEum"><span class="ne-text">4</span></h4><p id="ue4545bed" class="ne-p"><span class="ne-text" style="font-size: 12px">这是一个多重request的爬虫项目</span></p><pre data-language="python" id="xBpL4" class="ne-codeblock language-python"><code>from bs4 import BeautifulSoup
import requests
headers = {
    &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36&quot;  # 如果需要的话
}

if __name__ == '__main__':
    target = 'https://www.ypppt.com/moban/'
    req = requests.get(url = target,verify=False,headers=headers)
    req.encoding = 'utf-8'
    html = req.text
    soup = BeautifulSoup(html, &quot;lxml&quot;)
    namelist = soup.find_all(name = &quot;a&quot;,class_ = &quot;p-title&quot;)
    for link in namelist:
        ob = link.get(&quot;href&quot;)
        download1 = requests.get(url = f&quot;https://www.ypppt.com{ob}&quot;,verify=False,headers=headers)
        download1.encoding = 'utf-8'
        htmldl = download1.text
        soupdl = BeautifulSoup(htmldl, &quot;lxml&quot;)
        dllist = soupdl.find(name = &quot;a&quot;,class_ = &quot;down-button&quot;)
        downloadwebpagelink = dllist.get(&quot;href&quot;)
        download2 = requests.get(url = f&quot;https://www.ypppt.com{downloadwebpagelink}&quot;,verify=False,headers=headers)
        download2.encoding = 'utf-8'
        htmldl2 = download2.text
        soupdl2 = BeautifulSoup(htmldl2, &quot;lxml&quot;)
        dllist2 = soupdl2.find(name = &quot;a&quot;)
        dllink = dllist2.get(&quot;href&quot;)
        with open(&quot;./1117-1123/downloadlink.txt&quot;, &quot;a&quot;,encoding=&quot;utf-8&quot;) as file:
            if(dllink.find(&quot;https://down.ypppt.com/uploads&quot;) != -1):# 把外部链接删掉
                file.write(dllink+&quot;\n&quot;)

        # 我写的有点重复，可以封装成函数</code></pre><p id="u5ecd1a48" class="ne-p"><span class="ne-text" style="font-size: 12px">但第17个ppt模板是个外部链接，可以剔除。</span></p><p id="ud7234d4a" class="ne-p"><span class="ne-text" style="font-size: 12px">成果：</span><img src="https://cdn.nlark.com/yuque/0/2024/png/50039808/1732014242483-15cb8b90-ee57-446b-8c35-0543e551ed7a.png" width="951.3333333333334" id="ue04b50e3" class="ne-image"></p><h3 id="Tvbql"><span class="ne-text">本周情绪反馈</span></h3><ul class="ne-ul"><li id="uc6bf51f7" data-lake-index-type="0"><span class="ne-text">四个任务其实一开始是有难度的，主要是一开始没找对方向，以及没仔细看文档（任务一不知道怎么绕过反爬机制）。</span></li><li id="ubefa5e1f" data-lake-index-type="0"><span class="ne-text">感觉所谓的爬虫教程没那么重要，我直接看bs的官方文档（其实不如ai有效率）。有了大模型，具体问题具体分析更为重要。</span></li><li id="u90e968d5" data-lake-index-type="0"><span class="ne-text">任务1也告诉我不要吊死在一根树上，做项目就要用最简便的方法达到目的。</span></li><li id="u8f97339d" data-lake-index-type="0"><span class="ne-text">4个任务花了我3天，以后真要提高效率了</span><span class="ne-text">😥</span></li></ul></div>