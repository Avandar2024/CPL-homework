<!doctype html><div class="lake-content" typography="classic"><p id="u2cab5975" class="ne-p"><span class="ne-text">目前主要的问题是实践任务的任务文档2——爬取运动员信息网站。如果有哪位善良聪明富有爱心的老登看到了，还请不吝赐教指点</span></p><h2 id="jye5l"><span class="ne-text">必做任务1</span></h2><p id="u0342922a" class="ne-p"><span class="ne-text">我的感受是：</span></p><p id="uc3a9ff97" class="ne-p"><span class="ne-text">先回答老师的问题：</span></p><ul class="ne-ul"><li id="ub336dbae" data-lake-index-type="0"><span class="ne-text">问身边可能知道的人</span></li></ul><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="ue3347aa2" data-lake-index-type="0"><span class="ne-text">我怎么才能知道谁可能会知道我的问题         需要找到真正参与过或者了解过类似东西多人</span></li><li id="ua3c65f15" data-lake-index-type="0"><span class="ne-text">如果我问他，他会告诉我答案吗	            其实不一定，因为可能会存在利益关系等问题</span></li></ul></ul><ul class="ne-ul"><li id="uf3d778ef" data-lake-index-type="0"><span class="ne-text">问百度、bing、Google等搜索引擎</span></li></ul><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="u202713e8" data-lake-index-type="0"><span class="ne-text">搜索结果一大堆，哪一个才是我想要的，能解决问题的？      关注</span></li><li id="ua3952a76" data-lake-index-type="0"><span class="ne-text">有什么特定的技巧吗？    </span></li></ul></ul><ul class="ne-list-wrap"><ul class="ne-list-wrap"><ul ne-level="2" class="ne-ul"><li id="u25c51519" data-lake-index-type="0"><span class="ne-text">可以使用filetype等这种高级搜索语法</span></li><li id="ud56130b6" data-lake-index-type="0"><span class="ne-text">避免使用问句，使用关键词</span></li><li id="u45911163" data-lake-index-type="0"><span class="ne-text">关注时间、阅读量等等。关注是否为官网</span></li></ul></ul></ul><ul class="ne-ul"><li id="u48829d9a" data-lake-index-type="0"><span class="ne-text">问GPT等大模型</span></li></ul><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="ueca791f6" data-lake-index-type="0"><span class="ne-text">用起来挺爽，答案对吗？      不一定都对</span></li><li id="ue84525f6" data-lake-index-type="0"><span class="ne-text">通常情况下，大模型的输出都太空洞了，怎么办   我们可以加限定词</span></li></ul></ul><ul class="ne-ul"><li id="u6e061189" data-lake-index-type="0"><span class="ne-text">看B站、小红书等视频网站</span></li></ul><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="u4c0d6884" data-lake-index-type="0"><span class="ne-text">什么样的问题适合在视频网站上找    </span></li></ul></ul><ul class="ne-list-wrap"><ul class="ne-list-wrap"><ul ne-level="2" class="ne-ul"><li id="ubdc7d464" data-lake-index-type="0"><span class="ne-text">具体实操的，比如编程学习类/拆机的可以去b站</span></li><li id="u24b71ef5" data-lake-index-type="0"><span class="ne-text">旅游攻略 小红书</span></li><li id="u624f4c5c" data-lake-index-type="0"><span class="ne-text">数学 知乎</span></li><li id="uf2301dbd" data-lake-index-type="0"><span class="ne-text">不同的平台有不同的平台氛围，所以要去不同的平台找</span></li></ul></ul></ul><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="uf66b539c" data-lake-index-type="0"><span class="ne-text">不可能看完每个视频，怎么挑选视频</span></li></ul></ul><ul class="ne-list-wrap"><ul class="ne-list-wrap"><ul ne-level="2" class="ne-ul"><li id="u791db685" data-lake-index-type="0"><span class="ne-text">根据播放量，也可以扫一下下面的评论</span></li></ul></ul></ul><ul class="ne-ul"><li id="u9060f7d1" data-lake-index-type="0"><span class="ne-text">其他</span></li></ul><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="u81254319" data-lake-index-type="0"><span class="ne-text">还有其他我不知道的好的信息检索技巧吗？</span></li></ul></ul><ul class="ne-list-wrap"><ul class="ne-list-wrap"><ul ne-level="2" class="ne-ul"><li id="u670559c0" data-lake-index-type="0"><span class="ne-text">其实还可以利用qq群等社交群聊，但是往往这种信息会比较零散，不容易收集</span></li></ul></ul></ul><p id="u7ebd4127" class="ne-p"><br></p><p id="u00655833" class="ne-p"><span class="ne-text"></span></p><p id="uf30b015f" class="ne-p"><span class="ne-text">一个能力很强的人，他是如何进行信息检索的？</span></p><ul class="ne-ul"><li id="u9513d935" data-lake-index-type="0"><span class="ne-text">我个人认为无外乎上面的方法，但是我觉得有下面几个点会做的比别人好</span></li></ul><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="u4ce61bc3" data-lake-index-type="0"><span class="ne-text">第一他的信息渠道更广泛，比如他认识更多的人，有很多优质的群聊等</span></li><li id="u611ba1fd" data-lake-index-type="0"><span class="ne-text">第二他能够比较精确的定义问题，从而找到答案</span></li><li id="u6ad9fea0" data-lake-index-type="0"><span class="ne-text">第三他能够有比较好的辨别能力，或者说他的注意力比较好，能够比较敏感的注意到不同信息之间的矛盾点等等从而做出判断。或者说他善于进行取舍</span></li></ul></ul><p id="u42edf498" class="ne-p"><span class="ne-text"></span></p><p id="u6dad6e39" class="ne-p"><span class="ne-text"></span></p><p id="u85b4348b" class="ne-p"><span class="ne-text">任务耗时：10分钟</span></p><h2 id="Lw8HH"><span class="ne-text">必做任务2</span></h2><p id="u0522d62b" class="ne-p"><span class="ne-text">我的感受是：</span></p><ul class="ne-ul"><li id="u08557536" data-lake-index-type="0"><span class="ne-text">信息检索是一个很重要的能力，我们要掌握一些基本的搜索语法</span></li><li id="u8650fd4d" data-lake-index-type="0"><span class="ne-text">我们要找到合适的渠道，不同的东西可以去不同的平台去找，只有这样才能事倍功半（其实和我第一条里面提到的类似）</span></li></ul><p id="ud9c15e84" class="ne-p"><span class="ne-text"></span></p><p id="u25d33c68" class="ne-p"><span class="ne-text">任务耗时：10分钟</span></p><h2 id="SNqdv"><span class="ne-text">必做任务3</span></h2><p id="u29172084" class="ne-p"><span class="ne-text">我的感受是：</span></p><p id="u6836235d" class="ne-p"><span class="ne-text">编程就是把算法告诉计算机，把复杂的运算交给计算机处理从而解决问题的方法</span></p><p id="u4a422866" class="ne-p"><span class="ne-text">实际上本质是把人从繁重的计算中摆脱出来，我们人只需要思考</span></p><p id="ufcfa5be3" class="ne-p"><span class="ne-text"></span></p><p id="ubafa83d8" class="ne-p"><span class="ne-text">当然，对于除了计算机专业的同学，其他专业的同学也应该学会一些编程/电脑的使用技巧。下面附上我关注的两位NJU物理/商院的学长的公众号，后者甚至开发了南哪课表。</span></p><p id="uc089359c" class="ne-p"><a href="https://mp.weixin.qq.com/s/BaUtzYr_JQeZopXJyTGINA" data-href="https://mp.weixin.qq.com/s/BaUtzYr_JQeZopXJyTGINA" target="_blank" class="ne-link"><span class="ne-text">https://mp.weixin.qq.com/s/BaUtzYr_JQeZopXJyTGINA</span></a></p><p id="uaaa9582b" class="ne-p"><a href="https://mp.weixin.qq.com/s/Tf8hlqoXOcyq4d8DXo1z5A" data-href="https://mp.weixin.qq.com/s/Tf8hlqoXOcyq4d8DXo1z5A" target="_blank" class="ne-link"><span class="ne-text">https://mp.weixin.qq.com/s/Tf8hlqoXOcyq4d8DXo1z5A</span></a></p><p id="u26f52d62" class="ne-p"><span class="ne-text"></span></p><p id="u04e9739e" class="ne-p"><span class="ne-text"></span></p><p id="u1e45b342" class="ne-p"><span class="ne-text">任务耗时：10分钟</span></p><h2 id="wRKRa"><span class="ne-text">必做任务4</span></h2><p id="u2be66e5f" class="ne-p"><span class="ne-text">我学习了：</span></p><p id="u4d2adcf0" class="ne-p"><span class="ne-text">emm其实没有太学python的，其实有在学前端的一些内容。</span></p><p id="udc6fb64d" class="ne-p"><span class="ne-text">当然，跟着SICP的课程学习了面向对象等内容</span></p><p id="u829218a1" class="ne-p"><span class="ne-text">任务耗时：略，这个有点不好估计了</span></p><h2 id="EMeHW"><span class="ne-text">分支任务：</span></h2><h3 id="xL78A"><span class="ne-text">我选择完成：</span></h3><article class="lake-columns" style="display: flex"><article class="lake-column-item" style="flex: 0.33333333000000004"><ul class="ne-tl"><li id="uc4310cb5" data-lake-index-type="0"><input type="checkbox"><span class="ne-text">分支任务1</span></li><li id="u3a738b00" data-lake-index-type="0"><input type="checkbox"><span class="ne-text">分支任务2</span></li></ul></article><article class="lake-column-item" style="flex: 0.33333333000000004"><ul class="ne-tl"><li id="u4dfcf3ee" data-lake-index-type="0"><input type="checkbox"><span class="ne-text">分支任务3</span></li><li id="ub67fd927" data-lake-index-type="0"><input type="checkbox"><span class="ne-text">分支任务4</span></li></ul></article><article class="lake-column-item" style="flex: 0.33333333000000004"><ul class="ne-tl"><li id="u566fa63f" data-lake-index-type="0"><input type="checkbox"><span class="ne-text">分支任务5</span></li><li checked="true" id="ud8d6b74f" data-lake-index-type="0"><input type="checkbox" checked="true"><span class="ne-text">分支任务6</span></li></ul></article></article><h3 id="dVezT"><span class="ne-text">任务文档1 爬取教务网站：</span></h3><h6 id="C80S4"><span class="ne-text">第一次尝试</span></h6><ul class="ne-ul"><li id="ue958c13d" data-lake-index-type="0"><span class="ne-text">首先就是直接用request包做不了，经过cac提醒换了个包可以了</span></li><li id="ue3e1cc31" data-lake-index-type="0"><span class="ne-text">遇到bug了，我自己写的xml解析路径，在浏览器插件里面可以找出来，但是写到python就不行了,包括我用bs4的也会出问题，得不到想要的结果，怀疑南大有反扒</span></li><li id="u2b585719" data-lake-index-type="0"><span class="ne-text"></span></li></ul><p id="u1c05f455" class="ne-p"><img src="https://cdn.nlark.com/yuque/0/2024/png/46785276/1731935650648-32c12ac0-d931-4fef-946a-572dbd16ea78.png" width="1395.1515353109119" id="uc65de507" class="ne-image"></p><pre data-language="python" id="x9HYm" class="ne-codeblock language-python"><code>from curl_cffi import requests
from lxml import etree
base_url='https://jw.nju.edu.cn/'

headers = {
    &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36&quot;
}
response = requests.get(&quot;https://jw.nju.edu.cn&quot;,impersonate=&quot;chrome101&quot;)

content=response.text
# print(response.text)
html_tree = etree.HTML(content)
news_list=html_tree.xpath('//div[@class=&quot;post post1 post-41 mbox&quot;]//ul')#这一行的代码结果有返回
news_list=html_tree.xpath('//div[@class=&quot;post post1 post-41 mbox&quot;]//ul/li[@class=&quot;news n1 clearfix&quot;]')#这一行的代码结果没有返回

print(news_list)
for news in news_list:
    print(news)#.xpath('/span[@class=&quot;wjj&quot;]/text()')
</code></pre><p id="u95d13813" class="ne-p"><img src="https://cdn.nlark.com/yuque/0/2024/png/46785276/1731935636333-ede3f78f-c109-460f-9e09-47fef1611de3.png" width="1338.181837518025" id="ue85282b1" class="ne-image"></p><p id="u316c03cb" class="ne-p"><br></p><p id="u5945715d" class="ne-p"><img src="https://cdn.nlark.com/yuque/0/2024/png/46785276/1731935645098-571f6b0c-b4cc-4117-a6eb-65eb62efc3ae.png" width="1332.7272919846635" id="u6a8442aa" class="ne-image"></p><p id="uc4dec5d1" class="ne-p"><br></p><h6 id="aUPrl"><span class="ne-text">第二次尝试</span></h6><ul class="ne-ul"><li id="u17679c93" data-lake-index-type="0"><span class="ne-text">【第二天补充】破案了，是南大这个玩意不给我全的数据，真就被反扒了是吧</span></li><li id="u611fa2f8" data-lake-index-type="0"><span class="ne-text">胡恩齐同学提醒我主页的链接没有返回我要的新闻的部分，我才发现呢喃果然“人文关怀……，行政管理……”（笑）</span></li><li id="ue2570230" data-lake-index-type="0"><span class="ne-text">所以学长的反扒操作还得升级</span></li></ul><p id="u57cf8224" class="ne-p"><span class="ne-text">虽然但是，我加了请求头还不行啊！！！！！！</span></p><ul class="ne-ul"><li id="ub202e2f5" data-lake-index-type="0"><span class="ne-text">如果我加上'Content-Length': '784',这个就会一直跑，但是跑一会就挂了；不加就是直接返回数据告诉我拿不到数据</span></li></ul><pre data-language="python" id="TAUnU" class="ne-codeblock language-python"><code>from bs4 import BeautifulSoup
from curl_cffi import requests
from lxml import etree
base_url='https://jw.nju.edu.cn/_wp3services/generalQuery?queryObj=articles'

headers = {
    # 'Accept': 'application/json, text/javascript, */*; q=0.01',
    # 'Accept-Encoding': 'gzip, deflate, br, zstd',
    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6',
    'Connection': 'keep-alive',
    # 'Content-Length': '784',
    'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',
    'Cookie': '_ga_KDLLFKLFDS=GS1.1.1727710297.2.0.1727710297.0.0.0; _ga=GA1.3.1980588009.1727707845; _ga_2CQTNQR2QG=GS1.3.1730441561.1.0.1730441561.0.0.0; tfstk=fEmrdxb393Kzg5tCA8rUQoat-Jq8vuAsKDNQKvD3F7VlVL3e8AGURyDIRM0q1fb7-QHIgwcbMXGW9uHDmJFXegZCAT3x1AjSVUECYuELxCO6fhaSekEH-ISYJ7HmHJbU3lLIeYEdHt1H5qDRT5_90bqntoX0eRrhxa4k3t2_ig2hqkVD3Se3xMq3xtq0QRbhq8cnnKyLiWc2Dbhd1R4kCeBANE6L_zPinFIhYoj_z52zgMxS08-3sYVVxMV7oi9m3YOPXxZ-jfysNnjr_vDt-roFiCPK0xmo-xCyZ-37Lv2ajhBai0kE0z3fZa2a47ziqztMLY4numrSqBQthxPu7ogXkQaQ4buT6zYvMXk4NXmzo_RbOVMjqrlMGilL82m7m0ADbgWd9-4o1D3PtwzuH-P63KuiL6hxP9mdUwQLoP241LUhJwUuH-P63K7dJr203598-; _educoder_session=15a40e6cd0c12f6ac6f7886c0cfdeffa; www.nju.edu.cn=98184645; JSESSIONID=A65BDD4C4D03A7D7D0512D0205F89B3F',
    'Host': 'jw.nju.edu.cn',
    'Origin': 'https://jw.nju.edu.cn',
    'Referer': 'https://jw.nju.edu.cn/',
    # 'Sec-Fetch-Dest': 'empty',
    # 'Sec-Fetch-Mode': 'cors',
    'Sec-Fetch-Site': 'same-origin',
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36 Edg/131.0.0.0',
    'X-Requested-With': 'XMLHttpRequest',
    'sec-ch-ua': '&quot;Microsoft Edge&quot;;v=&quot;131&quot;, &quot;Chromium&quot;;v=&quot;131&quot;, &quot;Not_A Brand&quot;;v=&quot;24&quot;',
    'sec-ch-ua-mobile': '?0',
    # 'sec-ch-ua-platform': '&quot;Windows&quot;'
}

response = requests.get(base_url,impersonate=&quot;chrome101&quot;,headers=headers)

content=response.text
print(content)</code></pre><p id="u17953053" class="ne-p"><img src="https://cdn.nlark.com/yuque/0/2024/png/46785276/1731985987977-55e0bad0-61a4-41b2-acb7-9d2ff5d41fcd.png" width="1294.5454732511328" id="u72257e46" class="ne-image"></p><h6 id="RG4qh"><span class="ne-text">第三次尝试</span></h6><ul class="ne-ul"><li id="ub40eacd9" data-lake-index-type="0"><span class="ne-text">成功解决，本来把内容写到excel的，但是一个发现他返回的json数据格式不完全一样不好写，再一个标题行不全，所以就进入了子网页获得了所有的题目和时间，随手输出一下就好了</span></li></ul><p id="u355ac233" class="ne-p"><span class="ne-text"></span></p><pre data-language="python" id="D4lcn" class="ne-codeblock language-python"><code>import json
import pandas as pd
from curl_cffi import requests
from lxml import etree
def writeExcel_list( rows,title_rows,file_path):#这里要求输入的是一个列表，列表包含了若干个字典格式的数据
    if not rows:
        print(&quot;列表为空&quot;)
        return
    df = pd.DataFrame(rows,columns=title_rows)
    df.to_excel(file_path,index=False)
base_url='https://jw.nju.edu.cn/_wp3services/generalQuery?queryObj=articles'

headers = {
    # 'Accept': 'application/json, text/javascript, */*; q=0.01',
    # 'Accept-Encoding': 'gzip, deflate, br, zstd',
    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6',
    'Connection': 'keep-alive',
    # 'Content-Length': '784',
    'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',
    'Cookie': '_ga_KDLLFKLFDS=GS1.1.1727710297.2.0.1727710297.0.0.0; _ga=GA1.3.1980588009.1727707845; _ga_2CQTNQR2QG=GS1.3.1730441561.1.0.1730441561.0.0.0; tfstk=fEmrdxb393Kzg5tCA8rUQoat-Jq8vuAsKDNQKvD3F7VlVL3e8AGURyDIRM0q1fb7-QHIgwcbMXGW9uHDmJFXegZCAT3x1AjSVUECYuELxCO6fhaSekEH-ISYJ7HmHJbU3lLIeYEdHt1H5qDRT5_90bqntoX0eRrhxa4k3t2_ig2hqkVD3Se3xMq3xtq0QRbhq8cnnKyLiWc2Dbhd1R4kCeBANE6L_zPinFIhYoj_z52zgMxS08-3sYVVxMV7oi9m3YOPXxZ-jfysNnjr_vDt-roFiCPK0xmo-xCyZ-37Lv2ajhBai0kE0z3fZa2a47ziqztMLY4numrSqBQthxPu7ogXkQaQ4buT6zYvMXk4NXmzo_RbOVMjqrlMGilL82m7m0ADbgWd9-4o1D3PtwzuH-P63KuiL6hxP9mdUwQLoP241LUhJwUuH-P63K7dJr203598-; _educoder_session=15a40e6cd0c12f6ac6f7886c0cfdeffa; www.nju.edu.cn=98184645; JSESSIONID=A65BDD4C4D03A7D7D0512D0205F89B3F',
    'Host': 'jw.nju.edu.cn',
    'Origin': 'https://jw.nju.edu.cn',
    'Referer': 'https://jw.nju.edu.cn/',
    # 'Sec-Fetch-Dest': 'empty',
    # 'Sec-Fetch-Mode': 'cors',
    'Sec-Fetch-Site': 'same-origin',
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36 Edg/131.0.0.0',
    'X-Requested-With': 'XMLHttpRequest',
    'sec-ch-ua': '&quot;Microsoft Edge&quot;;v=&quot;131&quot;, &quot;Chromium&quot;;v=&quot;131&quot;, &quot;Not_A Brand&quot;;v=&quot;24&quot;',
    'sec-ch-ua-mobile': '?0',
    # 'sec-ch-ua-platform': '&quot;Windows&quot;'
}
data={
        &quot;queryObj&quot;: &quot;articles&quot;,
    'siteId': '414',
    'columnId': '26263',
    'pageIndex': '1',
    'rows': '12',
    'orders': '[{&quot;field&quot;:&quot;top&quot;,&quot;type&quot;:&quot;desc&quot;},{&quot;field&quot;:&quot;new&quot;,&quot;type&quot;:&quot;desc&quot;},{&quot;field&quot;:&quot;publishTime&quot;,&quot;type&quot;:&quot;desc&quot;}]',
    'returnInfos': '[{&quot;field&quot;:&quot;title&quot;,&quot;pattern&quot;:[{&quot;name&quot;:&quot;lp&quot;,&quot;value&quot;:&quot;23&quot;}],&quot;name&quot;:&quot;title&quot;},{&quot;field&quot;:&quot;f1&quot;,&quot;name&quot;:&quot;f1&quot;},{&quot;field&quot;:&quot;publishTime&quot;,&quot;pattern&quot;:[{&quot;name&quot;:&quot;d&quot;,&quot;value&quot;:&quot;MM-dd&quot;}],&quot;name&quot;:&quot;publishTime&quot;},{&quot;field&quot;:&quot;topImg&quot;,&quot;name&quot;:&quot;topImg&quot;},{&quot;field&quot;:&quot;newImg&quot;,&quot;name&quot;:&quot;newImg&quot;},{&quot;field&quot;:&quot;link&quot;,&quot;name&quot;:&quot;link&quot;}]'
}

response = requests.post(base_url,data=data,impersonate=&quot;chrome101&quot;,headers=headers)

content=response.text
response_data_js=json.loads(content)
response_data=dict(response_data_js)
news_data=response_data['data']
urls=[]
for news in news_data:
    # print(news)
    urls.append(news['url'])
# print(urls)
rows=[]
for url in urls:
    response = requests.post(url, impersonate=&quot;chrome101&quot;)
    html_tree = etree.HTML(response.text)
    try:
        title = html_tree.xpath('//div[@class=&quot;article&quot;]/h1/text()')[0]
        arti_publisher = html_tree.xpath('//div/p/span[@class=&quot;arti_publisher&quot;]/text()')[0]
        arti_update = html_tree.xpath('//div/p/span[@class=&quot;arti_update&quot;]/text()')[0]
        arti_views = html_tree.xpath('//div/p/span[@class=&quot;arti_views&quot;]/span/text()')[0]

        # print(&quot;得到一条&quot;)
        rows.append([title,arti_publisher,arti_update,arti_views])
    except:
        continue
print(rows)
writeExcel_list(rows,[&quot;title&quot;,&quot;arti_publisher&quot;,&quot;arti_update&quot;,&quot;arti_views&quot;],&quot;01news.xlsx&quot;)

#//div[@class=&quot;article&quot;]/h1/text()
</code></pre><p id="u28b3b897" class="ne-p"><span class="ne-text"></span></p><p id="ub5660fd6" class="ne-p"><span class="ne-text">就是有个非常弱智的错误，在保存excel的时候后缀名写成.excel了报了一堆错，服了</span></p><p id="u982a378a" class="ne-p"><span class="ne-text"></span></p><p id="u8e76dbf5" class="ne-p"><span class="ne-text"></span></p><p id="u0dad8359" class="ne-p"><span class="ne-text"></span></p><div id="bIPXw" class="ne-localdoc"><a href="https://nova.yuque.com/attachments/yuque/0/2024/xlsx/46785276/1732018909274-e61a0978-80c4-415a-bf19-9313fdc8ccc7.xlsx">01news.xlsx</a></div><p id="u1da9cfe4" class="ne-p"><span class="ne-text"></span></p><h4 id="UIMW4"><span class="ne-text">任务耗时：</span></h4><p id="u215352fd" class="ne-p"><span class="ne-text">3h</span></p><h3 id="PWwjz"><span class="ne-text">任务文档2 爬取运动员信息网站：</span></h3><p id="u4c6aa42e" class="ne-p"><a href="https://ydydj.univsport.com/level/" data-href="https://ydydj.univsport.com/level/" target="_blank" class="ne-link"><span class="ne-text">https://ydydj.univsport.com/level/</span></a></p><ul class="ne-ul"><li id="u657e27dc" data-lake-index-type="0"><span class="ne-text">最后的post url是</span><span class="ne-text" style="color: rgb(35, 25, 24); background-color: rgb(255, 251, 255); font-size: 12px">https://ydydj.univsport.com/api/system/athlete/front-end-list</span><span class="ne-text"> </span></li><li id="u9620d900" data-lake-index-type="0"><span class="ne-text">在左侧输入人名和证书后，点击下方按钮其实只有第一条链接会返回出来</span></li></ul><p id="u713ccc8b" class="ne-p"><span class="ne-text">	</span><img src="https://cdn.nlark.com/yuque/0/2024/png/46785276/1732020549417-0e42af53-d697-42a6-99d5-fe419559c75f.png" width="1298.1818369400405" id="uada1a98d" class="ne-image"><span class="ne-text"><br /></span></p><p id="u67325b8b" class="ne-p"><img src="https://cdn.nlark.com/yuque/0/2024/png/46785276/1732020476263-85e74530-8c42-4247-8404-8cf37b3e9f95.png" width="1206.6666841025308" id="u84a6b1c4" class="ne-image"></p><ul class="ne-ul"><li id="u4c609cdc" data-lake-index-type="0"><span class="ne-text">下面是十个测试数据</span></li><li id="uef0e548d" data-lake-index-type="0"><span class="ne-text">请输入cac的真实姓名（汉字）来获取，我后面的数据是通过excel表格读取的</span></li></ul><div id="Olq2M" class="ne-locked-text">此处为语雀加密文本卡片，点击链接查看：<a href="https://nova.yuque.com/bm61ur/unzgre/ny5oyzkswq5os4po#Olq2M">https://nova.yuque.com/bm61ur/unzgre/ny5oyzkswq5os4po#Olq2M</a></div><p id="ua542a862" class="ne-p"><span class="ne-text"></span></p><h6 id="XyF3d"><span class="ne-text">第一次尝试</span></h6><p id="ub7e5d145" class="ne-p"><br></p><p id="u13ff4d1f" class="ne-p"><span class="ne-text">这个网站也有反扒，目前还没搞定，鱼鱼了</span></p><p id="uc0275d2d" class="ne-p"><span class="ne-text">返回的都是    {&quot;code&quot;:500,&quot;data&quot;:null,&quot;msg&quot;:&quot;系统异常&quot;}</span></p><pre data-language="python" id="eZhrl" class="ne-codeblock language-python"><code>import requests
import pandas as pd
def readExcel(file_path):
    df = pd.read_excel(file_path)  # sheet_name不指定时默认返回全表数据

    # 获取所有的数据，返回的是一个list
    value = df.values
    return value
def getAthlete(target_url,AthleteValues):
    for ath in AthleteValues:
        certificateNo=ath[0]
        athleteRealName=ath[1]
        print(certificateNo)
        headers = {
            'accept': 'application/json, text/plain, */*',
            'accept-encoding': 'gzip, deflate, br, zstd',
            'accept-language': 'zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6',
            'content-length': '86',
            'content-type': 'application/json;charset=UTF-8',
            'origin': 'https://ydydj.univsport.com',
            'priority': 'u=1, i',
            'referer': 'https://ydydj.univsport.com/level/',
            'requestid': '012040399832',
            'sec-ch-ua': '&quot;Microsoft Edge&quot;;v=&quot;131&quot;, &quot;Chromium&quot;;v=&quot;131&quot;, &quot;Not_A Brand&quot;;v=&quot;24&quot;',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '&quot;Windows&quot;',
            'sec-fetch-dest': 'empty',
            'sec-fetch-mode': 'cors',
            'sec-fetch-site': 'same-origin',
            'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36 Edg/131.0.0.0'
        }

        data = {&quot;pageNo&quot;: 1,
                &quot;certificateNo&quot;: certificateNo,
                &quot;athleteRealName&quot;: athleteRealName,
                &quot;idCard&quot;: &quot;&quot;}

        response = requests.post(target_url, data=data,headers=headers)
        print(response.text)


values=readExcel('02demo.xlsx')
url = &quot;https://ydydj.univsport.com/api/system/athlete/front-end-list&quot;

getAthlete(url,values)</code></pre><h6 id="DqAVp"><span class="ne-text">第二次尝试</span></h6><ul class="ne-ul"><li id="uedfc754e" data-lake-index-type="0"><span class="ne-text">尝试了</span><span class="ne-text" style="color: rgb(38, 38, 38)">嵇煜人同学推荐的</span><span class="ne-text" style="color: #080808; background-color: #ffffff">DrissionPage，仍然不行，鱼鱼了，尝试了加入所有的headers，也不行</span></li></ul><pre data-language="python" id="LBp0u" class="ne-codeblock language-python"><code>import json
import pandas as pd
from curl_cffi import requests
from DrissionPage import SessionPage
from lxml import etree
def readExcel(file_path):
    df = pd.read_excel(file_path)  # sheet_name不指定时默认返回全表数据

    # 获取所有的数据，返回的是一个list
    value = df.values
    return value
def getAthlete(target_url,AthleteValues):
    for ath in AthleteValues:
        certificateNo=ath[0]
        athleteRealName=ath[1]
        headers = {
            'accept': 'application/json, text/plain, */*',
            'accept-encoding': 'gzip, deflate, br, zstd',
            'accept-language': 'zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6',
            'content-length': '86',
            'content-type': 'application/json;charset=UTF-8',
            'origin': 'https://ydydj.univsport.com',
            'priority': 'u=1, i',
            'referer': 'https://ydydj.univsport.com/level/',
            'requestid': '012040399832',
            'sec-ch-ua': '&quot;Microsoft Edge&quot;;v=&quot;131&quot;, &quot;Chromium&quot;;v=&quot;131&quot;, &quot;Not_A Brand&quot;;v=&quot;24&quot;',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '&quot;Windows&quot;',
            'sec-fetch-dest': 'empty',
            'sec-fetch-mode': 'cors',
            'sec-fetch-site': 'same-origin',
            'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36 Edg/131.0.0.0'
        }

        data = {&quot;pageNo&quot;: 1,
                &quot;certificateNo&quot;: certificateNo,
                &quot;athleteRealName&quot;: athleteRealName,
                &quot;idCard&quot;: &quot;&quot;}
        print(data)
        headers = {
            'Accept': 'application/json, text/javascript, */*; q=0.01',
            'Accept-Encoding': 'gzip, deflate, br, zstd',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6',
            'Connection': 'keep-alive',
            # 'Content-Length': '784',
            'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',
            'Cookie': '_ga_KDLLFKLFDS=GS1.1.1727710297.2.0.1727710297.0.0.0; _ga=GA1.3.1980588009.1727707845; _ga_2CQTNQR2QG=GS1.3.1730441561.1.0.1730441561.0.0.0; tfstk=fEmrdxb393Kzg5tCA8rUQoat-Jq8vuAsKDNQKvD3F7VlVL3e8AGURyDIRM0q1fb7-QHIgwcbMXGW9uHDmJFXegZCAT3x1AjSVUECYuELxCO6fhaSekEH-ISYJ7HmHJbU3lLIeYEdHt1H5qDRT5_90bqntoX0eRrhxa4k3t2_ig2hqkVD3Se3xMq3xtq0QRbhq8cnnKyLiWc2Dbhd1R4kCeBANE6L_zPinFIhYoj_z52zgMxS08-3sYVVxMV7oi9m3YOPXxZ-jfysNnjr_vDt-roFiCPK0xmo-xCyZ-37Lv2ajhBai0kE0z3fZa2a47ziqztMLY4numrSqBQthxPu7ogXkQaQ4buT6zYvMXk4NXmzo_RbOVMjqrlMGilL82m7m0ADbgWd9-4o1D3PtwzuH-P63KuiL6hxP9mdUwQLoP241LUhJwUuH-P63K7dJr203598-; _educoder_session=15a40e6cd0c12f6ac6f7886c0cfdeffa; www.nju.edu.cn=98184645; JSESSIONID=A65BDD4C4D03A7D7D0512D0205F89B3F',
            'Host': 'jw.nju.edu.cn',
            'Origin': 'https://jw.nju.edu.cn',
            'Referer': 'https://jw.nju.edu.cn/',
            'Sec-Fetch-Dest': 'empty',
            'Sec-Fetch-Mode': 'cors',
            'Sec-Fetch-Site': 'same-origin',
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36 Edg/131.0.0.0',
            'X-Requested-With': 'XMLHttpRequest',
            'sec-ch-ua': '&quot;Microsoft Edge&quot;;v=&quot;131&quot;, &quot;Chromium&quot;;v=&quot;131&quot;, &quot;Not_A Brand&quot;;v=&quot;24&quot;',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '&quot;Windows&quot;'
        }

        page = SessionPage()
        response=page.post(url, data=data,headers=headers)
        print(page.html)
        print(response)


values=readExcel('02demo.xlsx')
url = &quot;https://ydydj.univsport.com/api/system/athlete/front-end-list&quot;

getAthlete(url,values)
</code></pre><h6 id="uwAAM"><span class="ne-text">第三次尝试</span></h6><p id="u381f797d" class="ne-p"><span class="ne-text">cac给我换了个网站，但是我爬太快了给我封ip了</span></p><p id="uc13f938a" class="ne-p"><span class="ne-text">数据测试到了一个对的</span></p><p id="ud9326896" class="ne-p"><a href="https://zwfw.sport.gov.cn/level.html" data-href="https://zwfw.sport.gov.cn/level.html" target="_blank" class="ne-link"><span class="ne-text">https://zwfw.sport.gov.cn/level.html</span></a></p><pre data-language="python" id="LDnzD" class="ne-codeblock language-python"><code>import json
import time

import pandas as pd
from curl_cffi import requests


def readExcel(file_path):
    df = pd.read_excel(file_path)  # sheet_name不指定时默认返回全表数据

    # 获取所有的数据，返回的是一个list
    value = df.values
    return value
def getAthlete(target_url,AthleteValues):
    for ath in AthleteValues:
        certificateNo=ath[0]
        athleteRealName=ath[1]
        headers = {
            'accept': 'application/json, text/plain, */*',
            'accept-encoding': 'gzip, deflate, br, zstd',
            'accept-language': 'zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6',
            'content-length': '86',
            'content-type': 'application/json;charset=UTF-8',
            'origin': 'https://ydydj.univsport.com',
            'priority': 'u=1, i',
            'referer': 'https://ydydj.univsport.com/level/',
            'requestid': '012040399832',
            'sec-ch-ua': '&quot;Microsoft Edge&quot;;v=&quot;131&quot;, &quot;Chromium&quot;;v=&quot;131&quot;, &quot;Not_A Brand&quot;;v=&quot;24&quot;',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '&quot;Windows&quot;',
            'sec-fetch-dest': 'empty',
            'sec-fetch-mode': 'cors',
            'sec-fetch-site': 'same-origin',
            'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36 Edg/131.0.0.0'
        }

        data = {
            'm': 'getLevelList',
            'page': '1',
            'count': '10',
            'name': athleteRealName,
            'certificate_num': certificateNo,
            'rank': '',
            'item': '',
            'grant_starttime': '',
            'grant_endtime': ''
        }

        response = requests.post(url, data=data, impersonate=&quot;chrome101&quot;)
        time.sleep(1)
        print(response.text)


values=readExcel('02demo.xlsx')
url = &quot;https://zwfw.sport.gov.cn/level.do?m=getLevelList&quot;

getAthlete(url,values)</code></pre><h6 id="uzmRL"><span class="ne-text">第四次尝试</span></h6><p id="ued9d3aba" class="ne-p"><span class="ne-text">在胡恩齐同学的建议下，我又弄好了（用的第一个网站）。发现之前的data传错地方了hhh</span></p><pre data-language="python" id="dtlTn" class="ne-codeblock language-python"><code>import json
import time

import pandas as pd
import requests
from lxml import etree
def writeExcel_json(json_data, file_path):#这里要求输入的是一个列表，列表包含了若干个字典格式的数据
    if not json_data:
        return
    title_list=list(json_data[0].keys())
    rows=[]
    for row in json_data:
        rows.append(list(row.values()))
    df = pd.DataFrame(rows,columns=title_list)
    df.to_excel(file_path,index=False)

def readExcel(file_path):
    df = pd.read_excel(file_path)  # sheet_name不指定时默认返回全表数据
    # 获取所有的数据，返回的是一个list
    value = df.values
    return value


def getAthlete(target_url, AthleteValues):
    ath_jsons=[]
    count=0
    for ath in AthleteValues:
        count = count+1
        certificateNo = ath[1]
        athleteRealName = ath[0]
        data = {&quot;pageNo&quot;: &quot;1&quot;, &quot;certificateNo&quot;: str(certificateNo), &quot;athleteRealName&quot;: str(athleteRealName), &quot;idCard&quot;: &quot;&quot;}
        headers = {
            # ':authority': 'ydydj.univsport.com',
            # ':method': 'POST',
            # ':path': '/api/system/athlete/front-end-list',
            # ':scheme': 'https',
            # 'accept': 'application/json, text/plain, */*',
            # 'accept-encoding': 'gzip, deflate, br, zstd',
            # 'accept-language': 'zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6',
            # 'content-length': '86',
            # 'content-type': 'application/json;charset=UTF-8',
            # 'origin': 'https://ydydj.univsport.com',
            # 'priority': 'u=1, i',
            # 'referer': 'https://ydydj.univsport.com/level/',
            'requestid': '011353119219',
            # 'sec-ch-ua': '&quot;Microsoft Edge&quot;;v=&quot;131&quot;, &quot;Chromium&quot;;v=&quot;131&quot;, &quot;Not_A Brand&quot;;v=&quot;24&quot;',
            # 'sec-ch-ua-mobile': '?0',
            # 'sec-ch-ua-platform': '&quot;Windows&quot;',
            # 'sec-fetch-dest': 'empty',
            # 'sec-fetch-mode': 'cors',
            # 'sec-fetch-site': 'same-origin',
            'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36 Edg/131.0.0.0'
        }
        print(data)
        headers = {
            &quot;requestid&quot;: &quot;017939643192&quot;,
            &quot;user-agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36 Edg/131.0.0.0&quot;
        }
        response = requests.post(url, json=data, headers=headers)
        time.sleep(1)
        content = response.text
        json_data = json.loads(content)
        dic=dict(json_data)
        items=dic[&quot;data&quot;][&quot;list&quot;]
        if count==5:
            break
        ath_jsons.extend(items)
    return ath_jsons

values = readExcel('02demo.xlsx')
url = &quot;https://ydydj.univsport.com/api/system/athlete/front-end-list&quot;

ath_jsons=getAthlete(url, values)
writeExcel_json(ath_jsons, file_path='02Aths.xlsx')</code></pre><p id="ua33f1ffc" class="ne-p"><br></p><p id="u6fd4bc1b" class="ne-p"><br></p><h4 id="qRG1C"><span class="ne-text">任务耗时：</span></h4><p id="u9710973e" class="ne-p"><span class="ne-text">2.5h</span></p><h3 id="fLYHS"><span class="ne-text">任务文档3 爬取竞赛网站：</span></h3><ul class="ne-ul"><li id="ubc380b86" data-lake-index-type="0"><span class="ne-text">竞赛网站最友好，这是我最先完成的任务</span></li><li id="u3d15f7d7" data-lake-index-type="0"><span class="ne-text">写入excel的用的之前的，马上去学pandas</span></li><li id="u96160b47" data-lake-index-type="0"><span class="ne-text">甚至这里都用不上bs，因为他直接都给我json文件了</span></li><li id="uee6f0c72" data-lake-index-type="0"><span class="ne-text">我这里只测试了五页</span></li><li id="u16b417cf" data-lake-index-type="0"><span class="ne-text">最后的excel直接帮我写到该目录下的</span></li><li id="uee1950b4" data-lake-index-type="0"><span class="ne-text">我主要爬取了信息竞赛的</span></li></ul><pre data-language="python" id="V1frc" class="ne-codeblock language-python"><code>
import requests
import json
import xlwings as xw
def writeExcel(rows,file_name):

    TitleList = list(rows[0].keys())
    len_title = len(TitleList)
    with xw.App(visible=False) as app:  ## 创建一个Excel应用程序实例
        # 选择使用的工作簿
        book = app.books[0]
        # 选择使用的工作表
        sheet = book.sheets[0]
        # 在A1单元格写入标题行，效果是直接从左往右一次填充数据
        sheet.range('A1').value = TitleList
        for i in range(len(rows)):  # 遍历rows
            # 先获取该行数据,把字典改成列表
            r = list(rows[i].values())
            try:
                assert len_title == len(r)  # 保证数据行的数据个数和标题列数相同，否则就抛出异常
            except:
                print(&quot;在处理第{0}个所输入的行数据时出现不匹配情况，该数据为{1}，请再手动修改&quot;.format(i, r))
            finally:
                sheet.range('A' + str(i + 2)).value = r
        book.save(file_name)  # 保存文件


# https://www.cyscc.org/#/gs/winnerList?competitionId=105&amp;prizeId=11 主要是爬取了信息竞赛的
# 这个是生物竞赛的https://api-portal.cyscc.org/portal/api/gs/findUser?MOLAWARE=1&amp;competitionId=72&amp;prizeId=2&amp;page=1&amp;size=20，后面其实改一改就行了，这里是银牌的
# https://api-portal.cyscc.org/portal/api/gs/findUser?MOLAWARE=1&amp;competitionId=72&amp;prizeId=4&amp;page=1&amp;size=20

base_url='https://api-portal.cyscc.org/portal/api/gs/findUser?MOLAWARE=1&amp;competitionId=105&amp;prizeId=11&amp;'
# 其实只要修改competitionid和prizeid就能获取不同竞赛和不同学科不同类型的奖项，这里是2023年信息学竞赛的

headers = {
    &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36&quot;
}
flag=True#这个用来看是否到了最后一页
all_data=[]
page=1#从第一页开始
while flag:
    url=base_url+'page='+str(page)+'&amp;size=20'
    response = requests.get(url, headers=headers)
    content=response.text
    data=json.loads(content)
    data_dic=dict(data)
    data_list=data_dic['data']['list']
    print(data_list)
    all_data.extend(data_list)
    # print(data_dic['data']['list'])
    if not data_list:
        flag=False
    page+=1
    if page==5:
        break
    print(&quot;get a line&quot;)

writeExcel(all_data,'03Jingsai.xlsx')

</code></pre><ul class="ne-ul"><li id="ue636c613" data-lake-index-type="0"><span class="ne-text">下面的是用了pandas的写法，我爱pandas</span></li></ul><pre data-language="python" id="rZtYh" class="ne-codeblock language-python"><code>from bs4 import BeautifulSoup
import requests
import json
import pandas as pd
def writeExcel( json_data,file_path):#这里要求输入的是一个列表，列表包含了若干个字典格式的数据
    if not json_data:
        return
    title_list=list(json_data[0].keys())
    rows=[]
    for row in json_data:
        rows.append(list(row.values()))
    df = pd.DataFrame(rows,columns=title_list)
    df.to_excel(file_path,index=False)

# https://www.cyscc.org/#/gs/winnerList?competitionId=105&amp;prizeId=11 主要是爬取了信息竞赛的
# 这个是生物竞赛的https://api-portal.cyscc.org/portal/api/gs/findUser?MOLAWARE=1&amp;competitionId=72&amp;prizeId=2&amp;page=1&amp;size=20，后面其实改一改就行了，这里是银牌的
# https://api-portal.cyscc.org/portal/api/gs/findUser?MOLAWARE=1&amp;competitionId=72&amp;prizeId=4&amp;page=1&amp;size=20

base_url='https://api-portal.cyscc.org/portal/api/gs/findUser?MOLAWARE=1&amp;competitionId=105&amp;prizeId=11&amp;'
# 其实只要修改competitionid和prizeid就能获取不同竞赛和不同学科不同类型的奖项，这里是2023年信息学竞赛的

headers = {
    &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36&quot;
}
flag=True#这个用来看是否到了最后一页
all_data=[]
page=1#从第一页开始
while flag:
    url=base_url+'page='+str(page)+'&amp;size=20'
    response = requests.get(url, headers=headers)
    content=response.text
    data=json.loads(content)
    data_dic=dict(data)
    data_list=data_dic['data']['list']
    print(data_list)
    all_data.extend(data_list)
    # print(data_dic['data']['list'])
    if not data_list:
        flag=False
    page+=1
    if page==5:
        break
    print(&quot;get a line&quot;)
# print(all_data)
writeExcel(all_data,'03Jingsai_pandas.xlsx')


</code></pre><h4 id="zXN1u"><span class="ne-text">任务耗时：</span></h4><p id="u129ae6bd" class="ne-p"><span class="ne-text">50分钟+20（改用pandas）</span></p><p id="ua190ccd1" class="ne-p"><br></p><h3 id="twfWO"><span class="ne-text">任务文档4 爬取ppt网站的链接：</span></h3><p id="uc9944278" class="ne-p"><span class="ne-text">这个就是套娃和解套的过程，其实不难</span></p><p id="u4d864da9" class="ne-p"><span class="ne-text">最后的结果打印输出到控制台</span></p><p id="uffa55eb7" class="ne-p"><span class="ne-text">就是我怕封我ip，所以测试的时候只是爬取了5页的主页链接，最后只爬取了10个下载链接</span></p><pre data-language="python" id="TIa1n" class="ne-codeblock language-python"><code>
import requests
from lxml import etree


base_url='https://www.ypppt.com/moban/'
# 其实只要修改competitionid和prizeid就能获取不同竞赛和不同学科不同类型的奖项，这里是2023年信息学竞赛的

headers = {
    &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36&quot;
}
page = 1
flag=True
mainPageLinks=[]
while flag:
    if page==1:
        myurl=base_url
    else:
        myurl=base_url+&quot;list-&quot;+str(page)+&quot;.html&quot;
    response = requests.get(url=myurl, headers=headers)
    # print(response.text)
    #
    html_tree = etree.HTML(response.text)
    mainPageLinks.extend(html_tree.xpath('//ul[@class=&quot;posts clear&quot;]/li/a[@class=&quot;img_preview&quot;]/@href'))
    if page==5:
        break
    page+=1
# print(mainPageLinks)
# print(len(mainPageLinks))
base_url2='https://www.ypppt.com/'
mainPageLinks2=[]
# count =0
for lk in mainPageLinks:
    url2=base_url2+lk
    response = requests.get(url=url2, headers=headers)
    # print(response.text)
    #
    html_tree = etree.HTML(response.text)
    mainPageLinks2.extend(html_tree.xpath('//div[@class=&quot;button&quot;]/a/@href'))
    # print(&quot;get a item&quot;)
    # count +=1
    # if count==10:
    #     break
# print(mainPageLinks2)
base_url3='https://www.ypppt.com/'
mainPageLinks3=[]
# count =0
for lk2 in mainPageLinks2:
    url3=base_url3+lk2
    response = requests.get(url=url3, headers=headers)
    # print(response.text)
    #
    html_tree = etree.HTML(response.text)
    mainPageLinks3.append(html_tree.xpath('//ul/li/a/@href')[0])
    # if count==10:
    #     break
    # count+=1
    # print(&quot;get a item&quot;)

print(mainPageLinks3)




</code></pre><h4 id="leXWt"><span class="ne-text">任务耗时：</span></h4><p id="ue64f08cd" class="ne-p"><span class="ne-text">40分钟？</span></p><h3 id="fHddQ"><span class="ne-text">本周活动情绪反馈</span></h3><p id="u822f919e" class="ne-p"><span class="ne-text">爬虫还挺有意思的，就是为什么这么多反扒啊！！！！！！！！！！！！！！！！！！！！！！</span></p><ul class="ne-ul"><li id="u763cedfc" data-lake-index-type="0"><span class="ne-text">补充，发现自己还是菜了（，不是反扒是我技术够仔细了</span></li><li id="u7642ccdd" data-lake-index-type="0"><span class="ne-text">不过顺手学了pandas，感觉比我之前的xlwings确实一定程度好用而且快</span></li></ul><p id="uf856d2e3" class="ne-p"><br></p><p id="u1b2133cd" class="ne-p"><br></p></div>